---
title: æ·±å…¥ç†è§£Go-goroutineçš„å®ç°åŠè°ƒåº¦å™¨åˆ†æ
tags:
  - Go
categories:
  - Go
date: '2019-08-31 20:04'
abbrlink: 14010
---

åœ¨å­¦ä¹ Goçš„è¿‡ç¨‹ä¸­ï¼Œæœ€è®©äººæƒŠå¹çš„è«è¿‡äºgoroutineäº†ã€‚ä½†æ˜¯goroutineæ˜¯ä»€ä¹ˆï¼Œæˆ‘ä»¬ç”¨`go`å…³é”®å­—å°±å¯ä»¥åˆ›å»ºä¸€ä¸ªgoroutineï¼Œè¿™ä¹ˆå¤šçš„goroutineä¹‹é—´ï¼Œæ˜¯å¦‚ä½•è°ƒåº¦çš„å‘¢ï¼Ÿ

<!-- more -->

# ç»“æ„æ¦‚è§ˆ

åœ¨çœ‹Goæºç çš„è¿‡ç¨‹ä¸­ï¼Œéåœ°å¯è§gã€pã€mï¼Œæˆ‘ä»¬é¦–å…ˆå°±çœ‹ä¸€ä¸‹è¿™äº›å…³é”®å­—çš„ç»“æ„åŠç›¸äº’ä¹‹é—´çš„å…³ç³»

## æ•°æ®ç»“æ„

è¿™é‡Œæˆ‘ä»¬ä»…åˆ—å‡ºæ¥äº†ç»“æ„ä½“é‡Œé¢æ¯”è¾ƒå…³é”®çš„ä¸€äº›æˆå‘˜

### G(gouroutine)

goroutineæ˜¯è¿è¡Œæ—¶çš„æœ€å°æ‰§è¡Œå•å…ƒ

~~~go
type g struct {
	// Stack parameters.
	// stack describes the actual stack memory: [stack.lo, stack.hi).
	// stackguard0 is the stack pointer compared in the Go stack growth prologue.
	// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.
	// stackguard1 is the stack pointer compared in the C stack growth prologue.
	// It is stack.lo+StackGuard on g0 and gsignal stacks.
	// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).
  // å½“å‰gä½¿ç”¨çš„æ ˆç©ºé—´ï¼Œstackç»“æ„åŒ…æ‹¬ [lo, hi]ä¸¤ä¸ªæˆå‘˜
	stack       stack   // offset known to runtime/cgo
  // ç”¨äºæ£€æµ‹æ˜¯å¦éœ€è¦è¿›è¡Œæ ˆæ‰©å¼ ï¼Œgoä»£ç ä½¿ç”¨
	stackguard0 uintptr // offset known to liblink
  // ç”¨äºæ£€æµ‹æ˜¯å¦éœ€è¦è¿›è¡Œæ ˆæ‰©å±•ï¼ŒåŸç”Ÿä»£ç ä½¿ç”¨çš„
	stackguard1 uintptr // offset known to liblink
  // å½“å‰gæ‰€ç»‘å®šçš„m
	m              *m      // current m; offset known to arm liblink
  // å½“å‰gçš„è°ƒåº¦æ•°æ®ï¼Œå½“goroutineåˆ‡æ¢æ—¶ï¼Œä¿å­˜å½“å‰gçš„ä¸Šä¸‹æ–‡ï¼Œç”¨äºæ¢å¤
	sched          gobuf
	// gå½“å‰çš„çŠ¶æ€
	atomicstatus   uint32
  // å½“å‰gçš„id
	goid           int64
  // ä¸‹ä¸€ä¸ªgçš„åœ°å€ï¼Œé€šè¿‡guintptrç»“æ„ä½“çš„ptr setå‡½æ•°å¯ä»¥è®¾ç½®å’Œè·å–ä¸‹ä¸€ä¸ªgï¼Œé€šè¿‡è¿™ä¸ªå­—æ®µå’Œsched.gfreeStack sched.gfreeNoStack å¯ä»¥æŠŠ free gä¸²æˆä¸€ä¸ªé“¾è¡¨
	schedlink      guintptr
  // åˆ¤æ–­gæ˜¯å¦å…è®¸è¢«æŠ¢å 
	preempt        bool       // preemption signal, duplicates stackguard0 = stackpreempt
	// gæ˜¯å¦è¦æ±‚è¦å›åˆ°è¿™ä¸ªMæ‰§è¡Œ, æœ‰çš„æ—¶å€™gä¸­æ–­äº†æ¢å¤ä¼šè¦æ±‚ä½¿ç”¨åŸæ¥çš„Mæ‰§è¡Œ
	lockedm        muintptr
}
~~~

### P(process)

Pæ˜¯Mè¿è¡ŒGæ‰€éœ€çš„èµ„æº

```go
type p struct {
   lock mutex

   id          int32
   // pçš„çŠ¶æ€ï¼Œç¨åä»‹ç»
   status      uint32 // one of pidle/prunning/...
   // ä¸‹ä¸€ä¸ªpçš„åœ°å€ï¼Œå¯å‚è€ƒ g.schedlink
   link        puintptr
   // pæ‰€å…³è”çš„m
   m           muintptr   // back-link to associated m (nil if idle)
   // å†…å­˜åˆ†é…çš„æ—¶å€™ç”¨çš„ï¼Œpæ‰€å±çš„mçš„mcacheç”¨çš„ä¹Ÿæ˜¯è¿™ä¸ª
   mcache      *mcache
  
   // Cache of goroutine ids, amortizes accesses to runtimeÂ·sched.goidgen.
   // ä»schedä¸­è·å–å¹¶ç¼“å­˜çš„idï¼Œé¿å…æ¯æ¬¡åˆ†é…goidéƒ½ä»schedåˆ†é…
	 goidcache    uint64
	 goidcacheend uint64

   // Queue of runnable goroutines. Accessed without lock.
   // p æœ¬åœ°çš„runnbaleçš„goroutineå½¢æˆçš„é˜Ÿåˆ—
   runqhead uint32
   runqtail uint32
   runq     [256]guintptr
   // runnext, if non-nil, is a runnable G that was ready'd by
   // the current G and should be run next instead of what's in
   // runq if there's time remaining in the running G's time
   // slice. It will inherit the time left in the current time
   // slice. If a set of goroutines is locked in a
   // communicate-and-wait pattern, this schedules that set as a
   // unit and eliminates the (potentially large) scheduling
   // latency that otherwise arises from adding the ready'd
   // goroutines to the end of the run queue.
   // ä¸‹ä¸€ä¸ªæ‰§è¡Œçš„gï¼Œå¦‚æœæ˜¯nilï¼Œåˆ™ä»é˜Ÿåˆ—ä¸­è·å–ä¸‹ä¸€ä¸ªæ‰§è¡Œçš„g
   runnext guintptr

   // Available G's (status == Gdead)
   // çŠ¶æ€ä¸º Gdeadçš„gçš„åˆ—è¡¨ï¼Œå¯ä»¥è¿›è¡Œå¤ç”¨
   gfree    *g
   gfreecnt int32
}
```

### M(machine)

```go
type m struct {
   // g0æ˜¯ç”¨äºè°ƒåº¦å’Œæ‰§è¡Œç³»ç»Ÿè°ƒç”¨çš„ç‰¹æ®Šg
   g0      *g     // goroutine with scheduling stack
	 // må½“å‰è¿è¡Œçš„g
   curg          *g       // current running goroutine
   // å½“å‰æ‹¥æœ‰çš„p
   p             puintptr // attached p for executing go code (nil if not executing go code)
   // çº¿ç¨‹çš„ local storage
   tls           [6]uintptr   // thread-local storage
   // å”¤é†’mæ—¶ï¼Œmä¼šæ‹¥æœ‰è¿™ä¸ªp
   nextp         puintptr
   id            int64
   // å¦‚æœ !="", ç»§ç»­è¿è¡Œcurg
   preemptoff    string // if != "", keep curg running on this m
   // è‡ªæ—‹çŠ¶æ€ï¼Œç”¨äºåˆ¤æ–­mæ˜¯å¦å·¥ä½œå·²ç»“æŸï¼Œå¹¶å¯»æ‰¾gè¿›è¡Œå·¥ä½œ
   spinning      bool // m is out of work and is actively looking for work
   // ç”¨äºåˆ¤æ–­mæ˜¯å¦è¿›è¡Œä¼‘çœ çŠ¶æ€
   blocked       bool // m is blocked on a note
	 // mä¼‘çœ å’Œå”¤é†’é€šè¿‡è¿™ä¸ªï¼Œnoteé‡Œé¢æœ‰ä¸€ä¸ªæˆå‘˜keyï¼Œå¯¹è¿™ä¸ªkeyæ‰€æŒ‡å‘çš„åœ°å€è¿›è¡Œå€¼çš„ä¿®æ”¹ï¼Œè¿›è€Œè¾¾åˆ°å”¤é†’å’Œä¼‘çœ çš„ç›®çš„
   park          note
   // æ‰€æœ‰mç»„æˆçš„ä¸€ä¸ªé“¾è¡¨
   alllink       *m // on allm
   // ä¸‹ä¸€ä¸ªmï¼Œé€šè¿‡è¿™ä¸ªå­—æ®µå’Œsched.midle å¯ä»¥ä¸²æˆä¸€ä¸ªmçš„ç©ºé—²é“¾è¡¨
   schedlink     muintptr
   // mcacheï¼Œmæ‹¥æœ‰pçš„æ—¶å€™ï¼Œä¼šæŠŠè‡ªå·±çš„mcacheç»™p
   mcache        *mcache
   // lockedmçš„å¯¹åº”å€¼
   lockedg       guintptr
   // å¾…é‡Šæ”¾çš„mçš„listï¼Œé€šè¿‡sched.freem ä¸²æˆä¸€ä¸ªé“¾è¡¨
   freelink      *m      // on sched.freem
}
```

### sched

```go
type schedt struct {
   // å…¨å±€çš„go idåˆ†é…
   goidgen  uint64
   // è®°å½•çš„æœ€åä¸€æ¬¡ä»i/oä¸­æŸ¥è¯¢gçš„æ—¶é—´
   lastpoll uint64

   lock mutex

   // When increasing nmidle, nmidlelocked, nmsys, or nmfreed, be
   // sure to call checkdead().
	 // mçš„ç©ºé—²é“¾è¡¨ï¼Œç»“åˆm.schedlink å°±å¯ä»¥ç»„æˆä¸€ä¸ªç©ºé—²é“¾è¡¨äº†
   midle        muintptr // idle m's waiting for work
   nmidle       int32    // number of idle m's waiting for work
   nmidlelocked int32    // number of locked m's waiting for work
   // ä¸‹ä¸€ä¸ªmçš„idï¼Œä¹Ÿç”¨æ¥è®°å½•åˆ›å»ºçš„mæ•°é‡
   mnext        int64    // number of m's that have been created and next M ID
   // æœ€å¤šå…è®¸çš„mçš„æ•°é‡
   maxmcount    int32    // maximum number of m's allowed (or die)
   nmsys        int32    // number of system m's not counted for deadlock
   // freeæ‰çš„mçš„æ•°é‡ï¼Œexitçš„mçš„æ•°é‡
   nmfreed      int64    // cumulative number of freed m's

   ngsys uint32 // number of system goroutines; updated atomically

   pidle      puintptr // idle p's
   npidle     uint32
   nmspinning uint32 // See "Worker thread parking/unparking" comment in proc.go.

   // Global runnable queue.
   // è¿™ä¸ªå°±æ˜¯å…¨å±€çš„gçš„é˜Ÿåˆ—äº†ï¼Œå¦‚æœpçš„æœ¬åœ°é˜Ÿåˆ—æ²¡æœ‰gæˆ–è€…å¤ªå¤šï¼Œä¼šè·Ÿå…¨å±€é˜Ÿåˆ—è¿›è¡Œå¹³è¡¡
   // æ ¹æ®runqheadå¯ä»¥è·å–é˜Ÿåˆ—å¤´çš„gï¼Œç„¶åæ ¹æ®g.schedlink è·å–ä¸‹ä¸€ä¸ªï¼Œä»è€Œå½¢æˆäº†ä¸€ä¸ªé“¾è¡¨
   runqhead guintptr
   runqtail guintptr
   runqsize int32

   // freem is the list of m's waiting to be freed when their
   // m.exited is set. Linked through m.freelink.
   // ç­‰å¾…é‡Šæ”¾çš„mçš„åˆ—è¡¨
   freem *m
}
```

åœ¨è¿™é‡Œæ’ä¸€ä¸‹çŠ¶æ€çš„è§£æ

### g.status

- _Gidle: goroutineåˆšåˆšåˆ›å»ºè¿˜æ²¡æœ‰åˆå§‹åŒ–
- _Grunnable: goroutineå¤„äºè¿è¡Œé˜Ÿåˆ—ä¸­ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰è¿è¡Œï¼Œæ²¡æœ‰è‡ªå·±çš„æ ˆ
- _Grunning:  è¿™ä¸ªçŠ¶æ€çš„gå¯èƒ½å¤„äºè¿è¡Œç”¨æˆ·ä»£ç çš„è¿‡ç¨‹ä¸­ï¼Œæ‹¥æœ‰è‡ªå·±çš„må’Œp
- _Gsyscall: è¿è¡Œsystemcallä¸­
- _Gwaiting: è¿™ä¸ªçŠ¶æ€çš„goroutineæ­£åœ¨é˜»å¡ä¸­ï¼Œç±»ä¼¼äºç­‰å¾…channel
- _Gdead: è¿™ä¸ªçŠ¶æ€çš„gæ²¡æœ‰è¢«ä½¿ç”¨ï¼Œæœ‰å¯èƒ½æ˜¯åˆšåˆšé€€å‡ºï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯æ­£åœ¨åˆå§‹åŒ–ä¸­
- _Gcopystack: è¡¨ç¤ºgå½“å‰çš„æ ˆæ­£åœ¨è¢«ç§»é™¤ï¼Œæ–°æ ˆåˆ†é…ä¸­

### p.status

- _Pidle: ç©ºé—²çŠ¶æ€ï¼Œæ­¤æ—¶pä¸ç»‘å®šm
- _Prunning: mè·å–åˆ°pçš„æ—¶å€™ï¼Œpçš„çŠ¶æ€å°±æ˜¯è¿™ä¸ªçŠ¶æ€äº†ï¼Œç„¶åmå¯ä»¥ä½¿ç”¨è¿™ä¸ªpçš„èµ„æºè¿è¡Œg
- _Psyscall: å½“goè°ƒç”¨åŸç”Ÿä»£ç ï¼ŒåŸç”Ÿä»£ç åˆåè¿‡æ¥è°ƒç”¨goçš„æ—¶å€™ï¼Œä½¿ç”¨çš„på°±ä¼šå˜æˆæ­¤æ€
- _Pdead: å½“è¿è¡Œä¸­ï¼Œéœ€è¦å‡å°‘pçš„æ•°é‡æ—¶ï¼Œè¢«å‡æ‰çš„pçš„çŠ¶æ€å°±æ˜¯è¿™ä¸ªäº†

### m.status

mçš„statusæ²¡æœ‰pã€gçš„é‚£ä¹ˆæ˜ç¡®ï¼Œä½†æ˜¯åœ¨è¿è¡Œæµç¨‹çš„åˆ†æä¸­ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªçŠ¶æ€

- è¿è¡Œä¸­: æ‹¿åˆ°pï¼Œæ‰§è¡Œgçš„è¿‡ç¨‹ä¸­
- è¿è¡ŒåŸç”Ÿä»£ç : æ­£åœ¨æ‰§è¡ŒåŸå£°ä»£ç æˆ–è€…é˜»å¡çš„syscall
- ä¼‘çœ ä¸­: må‘ç°æ— å¾…è¿è¡Œçš„gæ—¶ï¼Œè¿›å…¥ä¼‘çœ ï¼Œå¹¶åŠ å…¥åˆ°ç©ºé—²åˆ—è¡¨ä¸­
- è‡ªæ—‹ä¸­(spining): å½“å‰å·¥ä½œç»“æŸï¼Œæ­£åœ¨å¯»æ‰¾ä¸‹ä¸€ä¸ªå¾…è¿è¡Œçš„g

åœ¨ä¸Šé¢çš„ç»“æ„ä¸­ï¼Œå­˜åœ¨å¾ˆå¤šçš„é“¾è¡¨ï¼Œg m pç»“æ„ä¸­è¿˜æœ‰æŒ‡å‘å¯¹æ–¹åœ°å€çš„æˆå‘˜ï¼Œé‚£ä¹ˆä»–ä»¬çš„å…³ç³»åˆ°åº•æ˜¯ä»€ä¹ˆæ ·çš„

![](http://note-1253518569.cossh.myqcloud.com/20190830155713.png)

æˆ‘ä»¬å¯ä»¥ä»ä¸Šå›¾ï¼Œç®€å•çš„è¡¨è¿°ä¸€ä¸‹ m p gçš„å…³ç³»

# æµç¨‹æ¦‚è§ˆ

ä»ä¸‹å›¾ï¼Œå¯ä»¥ç®€å•çš„ä¸€çª¥goçš„æ•´ä¸ªè°ƒåº¦æµç¨‹çš„å¤§æ¦‚

![](http://note-1253518569.cossh.myqcloud.com/20190830154901.png)

æ¥ä¸‹æ¥æˆ‘ä»¬å°±ä»æºç çš„è§’åº¦æ¥å…·ä½“çš„åˆ†ææ•´ä¸ªè°ƒåº¦æµç¨‹ï¼ˆæœ¬äººæ±‡ç¼–ä¸ç…§ï¼Œæ±‡ç¼–æ–¹é¢çš„å°±ä¸åˆ†æäº†ğŸ¤ªï¼‰

# æºç åˆ†æ

## åˆå§‹åŒ–

goçš„å¯åŠ¨æµç¨‹åˆ†ä¸º4æ­¥

1. call osinitï¼Œ è¿™é‡Œå°±æ˜¯è®¾ç½®äº†å…¨å±€å˜é‡ncpu = cpuæ ¸å¿ƒæ•°é‡
2. call schedinit
3. make & queue new G ï¼ˆruntime.newproc, go func()ä¹Ÿæ˜¯è°ƒç”¨è¿™ä¸ªå‡½æ•°æ¥åˆ›å»ºgoroutineï¼‰
4. call runtimeÂ·mstart

å…¶ä¸­ï¼Œschedinit å°±æ˜¯è°ƒåº¦å™¨çš„åˆå§‹åŒ–ï¼Œå‡ºå»schedinit ä¸­å¯¹å†…å­˜åˆ†é…ï¼Œåƒåœ¾å›æ”¶ç­‰æ“ä½œï¼Œé’ˆå¯¹è°ƒåº¦å™¨çš„åˆå§‹åŒ–å¤§è‡´å°±æ˜¯åˆå§‹åŒ–è‡ªèº«ï¼Œè®¾ç½®æœ€å¤§çš„maxmcountï¼Œ ç¡®å®špçš„æ•°é‡å¹¶åˆå§‹åŒ–è¿™äº›æ“ä½œ

### schedinit

schedinitè¿™é‡Œå¯¹å½“å‰mè¿›è¡Œäº†åˆå§‹åŒ–ï¼Œå¹¶æ ¹æ®osinitè·å–åˆ°çš„cpuæ ¸æ•°å’Œè®¾ç½®çš„`GOMAXPROCS` ç¡®å®špçš„æ•°é‡ï¼Œå¹¶è¿›è¡Œåˆå§‹åŒ–

```go
func schedinit() {
	// ä»TLSæˆ–è€…ä¸“ç”¨å¯„å­˜å™¨è·å–å½“å‰gçš„æŒ‡é’ˆç±»å‹
	_g_ := getg()
	// è®¾ç½®mæœ€å¤§çš„æ•°é‡
	sched.maxmcount = 10000

	// åˆå§‹åŒ–æ ˆçš„å¤ç”¨ç©ºé—´
	stackinit()
	// åˆå§‹åŒ–å½“å‰m
	mcommoninit(_g_.m)

	// osinitçš„æ—¶å€™ä¼šè®¾ç½® ncpuè¿™ä¸ªå…¨å±€å˜é‡ï¼Œè¿™é‡Œå°±æ˜¯æ ¹æ®cpuæ ¸å¿ƒæ•°å’Œå‚æ•°GOMAXPROCSæ¥ç¡®å®špçš„æ•°é‡
	procs := ncpu
	if n, ok := atoi32(gogetenv("GOMAXPROCS")); ok && n > 0 {
		procs = n
	}
	// ç”Ÿæˆè®¾å®šæ•°é‡çš„p
	if procresize(procs) != nil {
		throw("unknown runnable goroutine during bootstrap")
	}
}
```

### mcommoninit

```go
func mcommoninit(mp *m) {
	_g_ := getg()

	lock(&sched.lock)
	// åˆ¤æ–­mnextçš„å€¼æ˜¯å¦æº¢å‡ºï¼Œmnextéœ€è¦èµ‹å€¼ç»™m.id
	if sched.mnext+1 < sched.mnext {
		throw("runtime: thread ID overflow")
	}
	mp.id = sched.mnext
	sched.mnext++
	// åˆ¤æ–­mçš„æ•°é‡æ˜¯å¦æ¯”maxmcountè®¾å®šçš„è¦å¤šï¼Œå¦‚æœè¶…å‡ºç›´æ¥æŠ¥å¼‚å¸¸
	checkmcount()
	// åˆ›å»ºä¸€ä¸ªæ–°çš„gç”¨äºå¤„ç†signalï¼Œå¹¶åˆ†é…æ ˆ
	mpreinit(mp)
	if mp.gsignal != nil {
		mp.gsignal.stackguard1 = mp.gsignal.stack.lo + _StackGuard
	}

	// Add to allm so garbage collector doesn't free g->m
	// when it is just in a register or thread-local storage.
	// æ¥ä¸‹æ¥çš„ä¸¤è¡Œï¼Œé¦–å…ˆå°†å½“å‰mæ”¾åˆ°allmçš„å¤´ï¼Œç„¶ååŸå­æ“ä½œï¼Œå°†å½“å‰mçš„åœ°å€ï¼Œèµ‹å€¼ç»™mï¼Œè¿™æ ·å°±å°†å½“å‰mæ·»åŠ åˆ°äº†allmé“¾è¡¨çš„å¤´äº†
	mp.alllink = allm

	// NumCgoCall() iterates over allm w/o schedlock,
	// so we need to publish it safely.
	atomicstorep(unsafe.Pointer(&allm), unsafe.Pointer(mp))
	unlock(&sched.lock)

	// Allocate memory to hold a cgo traceback if the cgo call crashes.
	if iscgo || GOOS == "solaris" || GOOS == "windows" {
		mp.cgoCallers = new(cgoCallers)
	}
}
```

åœ¨è¿™é‡Œå°±å¼€å§‹æ¶‰åŠåˆ°äº†mé“¾è¡¨äº†ï¼Œè¿™ä¸ªé“¾è¡¨å¯ä»¥å¦‚ä¸‹å›¾è¡¨ç¤ºï¼Œå…¶ä»–çš„p gé“¾è¡¨å¯ä»¥å‚è€ƒï¼Œåªæ˜¯ä½¿ç”¨çš„ç»“æ„ä½“çš„å­—æ®µä¸ä¸€æ ·

#### 3.1.1.2. allmé“¾è¡¨ç¤ºæ„å›¾

![](http://note-1253518569.cossh.myqcloud.com/20190831105630.png)

#### 3.1.1.3. procresize

æ›´æ”¹pçš„æ•°é‡ï¼Œå¤šé€€å°‘è¡¥çš„åŸåˆ™ï¼Œåœ¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œç”±äºæœ€å¼€å§‹æ˜¯æ²¡æœ‰pçš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„ä½œç”¨å°±æ˜¯åˆå§‹åŒ–è®¾å®šæ•°é‡çš„päº†

`procesize` ä¸ä»…åœ¨åˆå§‹åŒ–çš„æ—¶å€™ä¼šè°ƒç”¨ï¼Œå½“ç”¨æˆ·æ‰‹åŠ¨è°ƒç”¨ `runtime.GOMAXPROCS` çš„æ—¶å€™ï¼Œä¼šé‡æ–°è®¾å®š nprocsï¼Œç„¶åæ‰§è¡Œ `startTheWorld()`ï¼Œ `startTheWorld()`ä¼šæ˜¯ä½¿ç”¨æ–°çš„ nprocs å†æ¬¡è°ƒç”¨`procresize` è¿™ä¸ªæ–¹æ³•

```go
func procresize(nprocs int32) *p {
	old := gomaxprocs
	if old < 0 || nprocs <= 0 {
		throw("procresize: invalid arg")
	}
	// update statistics
	now := nanotime()
	if sched.procresizetime != 0 {
		sched.totaltime += int64(old) * (now - sched.procresizetime)
	}
	sched.procresizetime = now

	// Grow allp if necessary.
	// å¦‚æœæ–°ç»™çš„pçš„æ•°é‡æ¯”åŸå…ˆçš„pçš„æ•°é‡å¤šï¼Œåˆ™æ–°å»ºå¢é•¿çš„p
	if nprocs > int32(len(allp)) {
		// Synchronize with retake, which could be running
		// concurrently since it doesn't run on a P.
		lock(&allpLock)
		// åˆ¤æ–­allp çš„capæ˜¯å¦æ»¡è¶³å¢é•¿åçš„é•¿åº¦ï¼Œæ»¡è¶³å°±ç›´æ¥ä½¿ç”¨ï¼Œä¸æ»¡è¶³ï¼Œåˆ™éœ€è¦æ‰©å¼ è¿™ä¸ªslice
		if nprocs <= int32(cap(allp)) {
			allp = allp[:nprocs]
		} else {
			nallp := make([]*p, nprocs)
			// Copy everything up to allp's cap so we
			// never lose old allocated Ps.
			copy(nallp, allp[:cap(allp)])
			allp = nallp
		}
		unlock(&allpLock)
	}

	// initialize new P's
	// åˆå§‹åŒ–æ–°å¢çš„p
	for i := int32(0); i < nprocs; i++ {
		pp := allp[i]
		if pp == nil {
			pp = new(p)
			pp.id = i
			pp.status = _Pgcstop
			pp.sudogcache = pp.sudogbuf[:0]
			for i := range pp.deferpool {
				pp.deferpool[i] = pp.deferpoolbuf[i][:0]
			}
			pp.wbBuf.reset()
			// allpæ˜¯ä¸€ä¸ªsliceï¼Œç›´æ¥å°†æ–°å¢çš„pæ”¾åˆ°å¯¹åº”çš„ç´¢å¼•ä¸‹é¢å°±okäº†
			atomicstorep(unsafe.Pointer(&allp[i]), unsafe.Pointer(pp))
		}
		if pp.mcache == nil {
			// åˆå§‹åŒ–æ—¶ï¼Œold=0ï¼Œç¬¬ä¸€ä¸ªæ–°å»ºçš„pç»™å½“å‰çš„mä½¿ç”¨
			if old == 0 && i == 0 {
				if getg().m.mcache == nil {
					throw("missing mcache?")
				}
				pp.mcache = getg().m.mcache // bootstrap
			} else {
				// ä¸ºpåˆ†é…å†…å­˜
				pp.mcache = allocmcache()
			}
		}
	}

	// free unused P's
	// é‡Šæ”¾æ‰å¤šä½™çš„pï¼Œå½“æ–°è®¾ç½®çš„pçš„æ•°é‡ï¼Œæ¯”åŸå…ˆè®¾å®šçš„pçš„æ•°é‡å°‘çš„æ—¶å€™ï¼Œä¼šèµ°åˆ°è¿™ä¸ªæµç¨‹
	// é€šè¿‡ runtime.GOMAXPROCS å°±å¯ä»¥åŠ¨æ€çš„ä¿®æ”¹nprocs
	for i := nprocs; i < old; i++ {
		p := allp[i]
		// move all runnable goroutines to the global queue
		// æŠŠå½“å‰pçš„è¿è¡Œé˜Ÿåˆ—é‡Œçš„gè½¬ç§»åˆ°å…¨å±€çš„gçš„é˜Ÿåˆ—
		for p.runqhead != p.runqtail {
			// pop from tail of local queue
			p.runqtail--
			gp := p.runq[p.runqtail%uint32(len(p.runq))].ptr()
			// push onto head of global queue
			globrunqputhead(gp)
		}
		// æŠŠrunnexté‡Œçš„gä¹Ÿè½¬ç§»åˆ°å…¨å±€é˜Ÿåˆ—
		if p.runnext != 0 {
			globrunqputhead(p.runnext.ptr())
			p.runnext = 0
		}
		// if there's a background worker, make it runnable and put
		// it on the global queue so it can clean itself up
		// å¦‚æœæœ‰gc workerçš„è¯ï¼Œä¿®æ”¹gçš„çŠ¶æ€ï¼Œç„¶åå†æŠŠå®ƒæ”¾åˆ°å…¨å±€é˜Ÿåˆ—ä¸­
		if gp := p.gcBgMarkWorker.ptr(); gp != nil {
			casgstatus(gp, _Gwaiting, _Grunnable)
			globrunqput(gp)
			// This assignment doesn't race because the
			// world is stopped.
			p.gcBgMarkWorker.set(nil)
		}
		// sudoigçš„bufå’Œcacheï¼Œä»¥åŠdeferpoolå…¨éƒ¨æ¸…ç©º
		for i := range p.sudogbuf {
			p.sudogbuf[i] = nil
		}
		p.sudogcache = p.sudogbuf[:0]
		for i := range p.deferpool {
			for j := range p.deferpoolbuf[i] {
				p.deferpoolbuf[i][j] = nil
			}
			p.deferpool[i] = p.deferpoolbuf[i][:0]
		}
		// é‡Šæ”¾æ‰å½“å‰pçš„mcache
		freemcache(p.mcache)
		p.mcache = nil
		// æŠŠå½“å‰pçš„gfreeè½¬ç§»åˆ°å…¨å±€
		gfpurge(p)
		// ä¿®æ”¹pçš„çŠ¶æ€ï¼Œè®©ä»–è‡ªç”Ÿè‡ªç­å»äº†
		p.status = _Pdead
		// can't free P itself because it can be referenced by an M in syscall
	}

	// Trim allp.
	if int32(len(allp)) != nprocs {
		lock(&allpLock)
		allp = allp[:nprocs]
		unlock(&allpLock)
	}
	// åˆ¤æ–­å½“å‰gæ˜¯å¦æœ‰pï¼Œæœ‰çš„è¯æ›´æ”¹å½“å‰ä½¿ç”¨çš„pçš„çŠ¶æ€ï¼Œç»§ç»­ä½¿ç”¨
	_g_ := getg()
	if _g_.m.p != 0 && _g_.m.p.ptr().id < nprocs {
		// continue to use the current P
		_g_.m.p.ptr().status = _Prunning
	} else {
		// release the current P and acquire allp[0]
		// å¦‚æœå½“å‰gæœ‰pï¼Œä½†æ˜¯æ‹¥æœ‰çš„æ˜¯å·²ç»é‡Šæ”¾çš„pï¼Œåˆ™ä¸å†ä½¿ç”¨è¿™ä¸ªpï¼Œé‡æ–°åˆ†é…
		if _g_.m.p != 0 {
			_g_.m.p.ptr().m = 0
		}
		// åˆ†é…allp[0]ç»™å½“å‰gä½¿ç”¨
		_g_.m.p = 0
		_g_.m.mcache = nil
		p := allp[0]
		p.m = 0
		p.status = _Pidle
		// å°†p m gç»‘å®šï¼Œå¹¶æŠŠm.mcacheæŒ‡å‘p.mcacheï¼Œå¹¶ä¿®æ”¹pçš„çŠ¶æ€ä¸º_Prunning
		acquirep(p)
	}
	var runnablePs *p
	for i := nprocs - 1; i >= 0; i-- {
		p := allp[i]
		if _g_.m.p.ptr() == p {
			continue
		}
		p.status = _Pidle
		// æ ¹æ® runqempty æ¥åˆ¤æ–­å½“å‰pçš„gè¿è¡Œé˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
		if runqempty(p) {
			// gè¿è¡Œé˜Ÿåˆ—ä¸ºç©ºçš„pï¼Œæ”¾åˆ° schedçš„pidleé˜Ÿåˆ—é‡Œé¢
			pidleput(p)
		} else {
			// g è¿è¡Œé˜Ÿåˆ—ä¸ä¸ºç©ºçš„pï¼Œç»„æˆä¸€ä¸ªå¯è¿è¡Œé˜Ÿåˆ—ï¼Œå¹¶æœ€åè¿”å›
			p.m.set(mget())
			p.link.set(runnablePs)
			runnablePs = p
		}
	}
	stealOrder.reset(uint32(nprocs))
	var int32p *int32 = &gomaxprocs // make compiler check that gomaxprocs is an int32
	atomic.Store((*uint32)(unsafe.Pointer(int32p)), uint32(nprocs))
	return runnablePs
}
```

- runqempty: è¿™ä¸ªå‡½æ•°æ¯”è¾ƒç®€å•ï¼Œå°±ä¸æ·±ç©¶äº†ï¼Œå°±æ˜¯æ ¹æ® p.runqtail == p.runqhead å’Œ p.runnext æ¥åˆ¤æ–­æœ‰æ²¡æœ‰å¾…è¿è¡Œçš„g
- pidleput: å°†å½“å‰çš„pè®¾ç½®ä¸º sched.pidleï¼Œç„¶åæ ¹æ®p.linkå°†ç©ºé—²pä¸²è”èµ·æ¥ï¼Œå¯å‚è€ƒä¸Šå›¾allmçš„é“¾è¡¨ç¤ºæ„å›¾

## ä»»åŠ¡

åˆ›å»ºä¸€ä¸ªgoroutineï¼Œåªéœ€è¦ä½¿ç”¨ `go func` å°±å¯ä»¥äº†ï¼Œç¼–è¯‘å™¨ä¼šå°†`go func` ç¿»è¯‘æˆ `newproc` è¿›è¡Œè°ƒç”¨ï¼Œé‚£ä¹ˆæ–°å»ºçš„ä»»åŠ¡æ˜¯å¦‚ä½•è°ƒç”¨çš„å‘¢ï¼Œæˆ‘ä»¬ä»åˆ›å»ºå¼€å§‹è¿›è¡Œè·Ÿè¸ª

### newproc

`newproc` å‡½æ•°è·å–äº†å‚æ•°å’Œå½“å‰gçš„pcä¿¡æ¯ï¼Œå¹¶é€šè¿‡g0è°ƒç”¨`newproc1`å»çœŸæ­£çš„æ‰§è¡Œåˆ›å»ºæˆ–è·å–å¯ç”¨çš„g

```go
func newproc(siz int32, fn *funcval) {
	// è·å–ç¬¬ä¸€å‚æ•°åœ°å€
	argp := add(unsafe.Pointer(&fn), sys.PtrSize)
	// è·å–å½“å‰æ‰§è¡Œçš„g
	gp := getg()
	// è·å–å½“å‰gçš„pc
	pc := getcallerpc()
	systemstack(func() {
		// ä½¿ç”¨g0å»æ‰§è¡Œnewproc1å‡½æ•°
		newproc1(fn, (*uint8)(argp), siz, gp, pc)
	})
}
```

### newproc1

newporc1 çš„ä½œç”¨å°±æ˜¯åˆ›å»ºæˆ–è€…è·å–ä¸€ä¸ªç©ºé—´çš„gï¼Œåˆå§‹åŒ–è¿™ä¸ªgï¼Œå¹¶å°è¯•å¯»æ‰¾ä¸€ä¸ªpå’Œmå»æ‰§è¡Œg

```go
func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) {
	_g_ := getg()

	if fn == nil {
		_g_.m.throwing = -1 // do not dump full stacks
		throw("go of nil func value")
	}
	// åŠ é”ç¦æ­¢è¢«æŠ¢å 
	_g_.m.locks++ // disable preemption because it can be holding p in a local var
	siz := narg
	siz = (siz + 7) &^ 7

	// We could allocate a larger initial stack if necessary.
	// Not worth it: this is almost always an error.
	// 4*sizeof(uintreg): extra space added below
	// sizeof(uintreg): caller's LR (arm) or return address (x86, in gostartcall).
	// å¦‚æœå‚æ•°è¿‡å¤šï¼Œåˆ™ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œæ ˆå¤§å°æ˜¯2k
	if siz >= _StackMin-4*sys.RegSize-sys.RegSize {
		throw("newproc: function arguments too large for new goroutine")
	}

	_p_ := _g_.m.p.ptr()
	// å°è¯•è·å–ä¸€ä¸ªç©ºé—²çš„gï¼Œå¦‚æœè·å–ä¸åˆ°ï¼Œåˆ™æ–°å»ºä¸€ä¸ªï¼Œå¹¶æ·»åŠ åˆ°allgé‡Œé¢
	// gfgeté¦–å…ˆä¼šå°è¯•ä»pæœ¬åœ°è·å–ç©ºé—²çš„gï¼Œå¦‚æœæœ¬åœ°æ²¡æœ‰çš„è¯ï¼Œåˆ™ä»å…¨å±€è·å–ä¸€å †å¹³è¡¡åˆ°æœ¬åœ°p
	newg := gfget(_p_)
	if newg == nil {
		newg = malg(_StackMin)
		casgstatus(newg, _Gidle, _Gdead)
		// æ–°å»ºçš„gï¼Œæ·»åŠ åˆ°å…¨å±€çš„ allgé‡Œé¢ï¼Œallgæ˜¯ä¸€ä¸ªsliceï¼Œ appendè¿›å»å³å¯
		allgadd(newg) // publishes with a g->status of Gdead so GC scanner doesn't look at uninitialized stack.
	}
	// åˆ¤æ–­è·å–çš„gçš„æ ˆæ˜¯å¦æ­£å¸¸
	if newg.stack.hi == 0 {
		throw("newproc1: newg missing stack")
	}
	// åˆ¤æ–­gçš„çŠ¶æ€æ˜¯å¦æ­£å¸¸
	if readgstatus(newg) != _Gdead {
		throw("newproc1: new g is not Gdead")
	}
	// é¢„ç•™ä¸€ç‚¹ç©ºé—´ï¼Œé˜²æ­¢è¯»å–è¶…å‡ºä¸€ç‚¹ç‚¹
	totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame
	// ç©ºé—´å¤§å°è¿›è¡Œå¯¹é½
	totalSize += -totalSize & (sys.SpAlign - 1) // align to spAlign
	sp := newg.stack.hi - totalSize
	spArg := sp
	// usesLr ä¸º0ï¼Œè¿™é‡Œä¸æ‰§è¡Œ
	if usesLR {
		// caller's LR
		*(*uintptr)(unsafe.Pointer(sp)) = 0
		prepGoExitFrame(sp)
		spArg += sys.MinFrameSize
	}
	if narg > 0 {
		// å°†å‚æ•°æ‹·è´å…¥æ ˆ
		memmove(unsafe.Pointer(spArg), unsafe.Pointer(argp), uintptr(narg))
		// ... çœç•¥ ...
	}
	// åˆå§‹åŒ–ç”¨äºä¿å­˜ç°åœºçš„åŒºåŸŸåŠåˆå§‹åŒ–åŸºæœ¬çŠ¶æ€
	memclrNoHeapPointers(unsafe.Pointer(&newg.sched), unsafe.Sizeof(newg.sched))
	newg.sched.sp = sp
	newg.stktopsp = sp
	// è¿™é‡Œä¿å­˜äº†goexitçš„åœ°å€ï¼Œåœ¨ç”¨æˆ·å‡½æ•°æ‰§è¡Œå®Œæˆåï¼Œä¼šæ ¹æ®pcæ¥æ‰§è¡Œgoexit
	newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
	newg.sched.g = guintptr(unsafe.Pointer(newg))
	// è¿™é‡Œè°ƒæ•´ sched ä¿¡æ¯ï¼Œpc = goexitçš„åœ°å€
	gostartcallfn(&newg.sched, fn)
	newg.gopc = callerpc
	newg.ancestors = saveAncestors(callergp)
	newg.startpc = fn.fn
	if _g_.m.curg != nil {
		newg.labels = _g_.m.curg.labels
	}
	if isSystemGoroutine(newg) {
		atomic.Xadd(&sched.ngsys, +1)
	}
	newg.gcscanvalid = false
	casgstatus(newg, _Gdead, _Grunnable)
	// å¦‚æœpç¼“å­˜çš„goidå·²ç»ç”¨å®Œï¼Œæœ¬åœ°å†ä»schedæ‰¹é‡è·å–ä¸€ç‚¹
	if _p_.goidcache == _p_.goidcacheend {
		// Sched.goidgen is the last allocated id,
		// this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch].
		// At startup sched.goidgen=0, so main goroutine receives goid=1.
		_p_.goidcache = atomic.Xadd64(&sched.goidgen, _GoidCacheBatch)
		_p_.goidcache -= _GoidCacheBatch - 1
		_p_.goidcacheend = _p_.goidcache + _GoidCacheBatch
	}
	// åˆ†é…goid
	newg.goid = int64(_p_.goidcache)
	_p_.goidcache++
	// æŠŠæ–°çš„gæ”¾åˆ° p çš„å¯è¿è¡Œgé˜Ÿåˆ—ä¸­
	runqput(_p_, newg, true)
	// åˆ¤æ–­æ˜¯å¦æœ‰ç©ºé—²pï¼Œä¸”æ˜¯å¦éœ€è¦å”¤é†’ä¸€ä¸ªmæ¥æ‰§è¡Œg
	if atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 && mainStarted {
		wakep()
	}
	_g_.m.locks--
	if _g_.m.locks == 0 && _g_.preempt { // restore the preemption request in case we've cleared it in newstack
		_g_.stackguard0 = stackPreempt
	}
}
```

#### gfget

è¿™ä¸ªå‡½æ•°çš„é€»è¾‘æ¯”è¾ƒç®€å•ï¼Œå°±æ˜¯çœ‹ä¸€ä¸‹pæœ‰æ²¡æœ‰ç©ºé—²çš„gï¼Œæ²¡æœ‰åˆ™å»å…¨å±€çš„freegé˜Ÿåˆ—æŸ¥æ‰¾ï¼Œè¿™é‡Œå°±æ¶‰åŠäº†pæœ¬åœ°å’Œå…¨å±€å¹³è¡¡çš„ä¸€ä¸ªäº¤äº’äº†

```go
func gfget(_p_ *p) *g {
retry:
	gp := _p_.gfree
	// æœ¬åœ°çš„gé˜Ÿåˆ—ä¸ºç©ºï¼Œä¸”å…¨å±€é˜Ÿåˆ—ä¸ä¸ºç©ºï¼Œåˆ™ä»å…¨å±€é˜Ÿåˆ—ä¸€æ¬¡è·å–è‡³å¤š32ä¸ªä¸‹æ¥ï¼Œå¦‚æœå…¨å±€é˜Ÿåˆ—ä¸å¤Ÿå°±ç®—äº†
	if gp == nil && (sched.gfreeStack != nil || sched.gfreeNoStack != nil) {
		lock(&sched.gflock)
		for _p_.gfreecnt < 32 {
			if sched.gfreeStack != nil {
				// Prefer Gs with stacks.
				gp = sched.gfreeStack
				sched.gfreeStack = gp.schedlink.ptr()
			} else if sched.gfreeNoStack != nil {
				gp = sched.gfreeNoStack
				sched.gfreeNoStack = gp.schedlink.ptr()
			} else {
				break
			}
			_p_.gfreecnt++
			sched.ngfree--
			gp.schedlink.set(_p_.gfree)
			_p_.gfree = gp
		}
		// å·²ç»ä»å…¨å±€æ‹¿äº†gäº†ï¼Œå†å»ä»å¤´å¼€å§‹åˆ¤æ–­
		unlock(&sched.gflock)
		goto retry
	}
	// å¦‚æœæ‹¿åˆ°äº†gï¼Œåˆ™åˆ¤æ–­gæ˜¯å¦æœ‰æ ˆï¼Œæ²¡æœ‰æ ˆå°±åˆ†é…
	// æ ˆçš„åˆ†é…è·Ÿå†…å­˜åˆ†é…å·®ä¸å¤šï¼Œé¦–å…ˆåˆ›å»ºå‡ ä¸ªå›ºå®šå¤§å°çš„æ ˆçš„æ•°ç»„ï¼Œç„¶ååˆ°æŒ‡å®šå¤§å°çš„æ•°ç»„é‡Œé¢å»åˆ†é…å°±okäº†ï¼Œè¿‡å¤§åˆ™ç›´æ¥å…¨å±€åˆ†é…
	if gp != nil {
		_p_.gfree = gp.schedlink.ptr()
		_p_.gfreecnt--
		if gp.stack.lo == 0 {
			// Stack was deallocated in gfput. Allocate a new one.
			systemstack(func() {
				gp.stack = stackalloc(_FixedStack)
			})
			gp.stackguard0 = gp.stack.lo + _StackGuard
		} else {
			// ... çœç•¥ ...
		}
	}
	// æ³¨æ„ï¼š å¦‚æœå…¨å±€æ²¡æœ‰gï¼Œpä¹Ÿæ²¡æœ‰gï¼Œåˆ™è¿”å›çš„gpè¿˜æ˜¯nil
	return gp
}
```

#### runqput

runqputä¼šæŠŠgæ”¾åˆ°pçš„æœ¬åœ°é˜Ÿåˆ—æˆ–è€…p.runnextï¼Œå¦‚æœpçš„æœ¬åœ°é˜Ÿåˆ—è¿‡é•¿ï¼Œåˆ™æŠŠgåˆ°å…¨å±€é˜Ÿåˆ—ï¼ŒåŒæ—¶å¹³è¡¡pæœ¬åœ°é˜Ÿåˆ—çš„ä¸€åŠåˆ°å…¨å±€

```go
func runqput(_p_ *p, gp *g, next bool) {
	if randomizeScheduler && next && fastrand()%2 == 0 {
		next = false
	}
	// å¦‚æœnextä¸ºtrueï¼Œåˆ™æ”¾å…¥åˆ°p.runnexté‡Œé¢ï¼Œå¹¶æŠŠåŸå…ˆrunnextçš„gäº¤æ¢å‡ºæ¥
	if next {
	retryNext:
		oldnext := _p_.runnext
		if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) {
			goto retryNext
		}
		if oldnext == 0 {
			return
		}
		// Kick the old runnext out to the regular run queue.
		gp = oldnext.ptr()
	}

retry:
	h := atomic.Load(&_p_.runqhead) // load-acquire, synchronize with consumers
	t := _p_.runqtail
	// åˆ¤æ–­pçš„é˜Ÿåˆ—çš„é•¿åº¦æ˜¯å¦è¶…äº†ï¼Œ runqæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º256çš„æ•°ç»„ï¼Œè¶…å‡ºçš„è¯å°±ä¼šæ”¾åˆ°å…¨å±€é˜Ÿåˆ—äº†
	if t-h < uint32(len(_p_.runq)) {
		_p_.runq[t%uint32(len(_p_.runq))].set(gp)
		atomic.Store(&_p_.runqtail, t+1) // store-release, makes the item available for consumption
		return
	}
	// æŠŠgæ”¾åˆ°å…¨å±€é˜Ÿåˆ—
	if runqputslow(_p_, gp, h, t) {
		return
	}
	// the queue is not full, now the put above must succeed
	goto retry
}
```

#### runqputslow

```go
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {
	var batch [len(_p_.runq)/2 + 1]*g

	// First, grab a batch from local queue.
	n := t - h
	n = n / 2
	if n != uint32(len(_p_.runq)/2) {
		throw("runqputslow: queue is not full")
	}
	// è·å–påé¢çš„ä¸€åŠ
	for i := uint32(0); i < n; i++ {
		batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr()
	}
	if !atomic.Cas(&_p_.runqhead, h, h+n) { // cas-release, commits consume
		return false
	}
	batch[n] = gp

	// Link the goroutines.
	for i := uint32(0); i < n; i++ {
		batch[i].schedlink.set(batch[i+1])
	}

	// Now put the batch on global queue.
	// æ”¾åˆ°å…¨å±€é˜Ÿåˆ—é˜Ÿå°¾
	lock(&sched.lock)
	globrunqputbatch(batch[0], batch[n], int32(n+1))
	unlock(&sched.lock)
	return true
}
```

æ–°å»ºä»»åŠ¡è‡³æ­¤åŸºæœ¬ç»“æŸï¼Œåˆ›å»ºå®Œæˆä»»åŠ¡åï¼Œç­‰å¾…è°ƒåº¦æ‰§è¡Œå°±å¥½äº†ï¼Œä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œä»»åŠ¡çš„ä¼˜å…ˆçº§æ˜¯ p.runnext > p.runq > sched.runq

gä»åˆ›å»ºåˆ°æ‰§è¡Œç»“æŸå¹¶æ”¾å…¥freeé˜Ÿåˆ—ä¸­çš„çŠ¶æ€è½¬æ¢å¤§è‡´å¦‚ä¸‹å›¾æ‰€ç¤º

![](http://note-1253518569.cossh.myqcloud.com/20190831160849.png)

### wakep

å½“ newproc1åˆ›å»ºå®Œä»»åŠ¡åï¼Œä¼šå°è¯•å”¤é†’mæ¥æ‰§è¡Œä»»åŠ¡

```go
func wakep() {
	// be conservative about spinning threads
	// ä¸€æ¬¡åº”è¯¥åªæœ‰ä¸€ä¸ªmåœ¨spiningï¼Œå¦åˆ™å°±é€€å‡º
	if !atomic.Cas(&sched.nmspinning, 0, 1) {
		return
	}
	// è°ƒç”¨startmæ¥æ‰§è¡Œ
	startm(nil, true)
}
```

### startm

è°ƒåº¦mæˆ–è€…åˆ›å»ºmæ¥è¿è¡Œpï¼Œå¦‚æœp==nilï¼Œå°±ä¼šå°è¯•è·å–ä¸€ä¸ªç©ºé—²pï¼Œpçš„é˜Ÿåˆ—ä¸­æœ‰gï¼Œæ‹¿åˆ°påæ‰èƒ½æ‹¿åˆ°g

```go
func startm(_p_ *p, spinning bool) {
	lock(&sched.lock)
	if _p_ == nil {
		// å¦‚æœæ²¡æœ‰æŒ‡å®šp, åˆ™ä»sched.pidleè·å–ç©ºé—²çš„p
		_p_ = pidleget()
		if _p_ == nil {
			unlock(&sched.lock)
			// å¦‚æœæ²¡æœ‰è·å–åˆ°pï¼Œé‡ç½®nmspinning
			if spinning {
				// The caller incremented nmspinning, but there are no idle Ps,
				// so it's okay to just undo the increment and give up.
				if int32(atomic.Xadd(&sched.nmspinning, -1)) < 0 {
					throw("startm: negative nmspinning")
				}
			}
			return
		}
	}
	// é¦–å…ˆå°è¯•ä» sched.midleè·å–ä¸€ä¸ªç©ºé—²çš„m
	mp := mget()
	unlock(&sched.lock)
	if mp == nil {
		// å¦‚æœè·å–ä¸åˆ°ç©ºé—²çš„mï¼Œåˆ™åˆ›å»ºä¸€ä¸ª mspining = trueçš„mï¼Œå¹¶å°†pç»‘å®šåˆ°mä¸Šï¼Œç›´æ¥è¿”å›
		var fn func()
		if spinning {
			// The caller incremented nmspinning, so set m.spinning in the new M.
			fn = mspinning
		}
		newm(fn, _p_)
		return
	}
	// åˆ¤æ–­è·å–åˆ°çš„ç©ºé—²mæ˜¯å¦æ˜¯spiningçŠ¶æ€
	if mp.spinning {
		throw("startm: m is spinning")
	}
	// åˆ¤æ–­è·å–åˆ°çš„mæ˜¯å¦æœ‰p
	if mp.nextp != 0 {
		throw("startm: m has p")
	}
	if spinning && !runqempty(_p_) {
		throw("startm: p has runnable gs")
	}
	// The caller incremented nmspinning, so set m.spinning in the new M.
	// è°ƒç”¨å‡½æ•°çš„çˆ¶å‡½æ•°å·²ç»å¢åŠ äº†nmspinningï¼Œ è¿™é‡Œåªéœ€è¦è®¾ç½®m.spiningå°±okäº†ï¼ŒåŒæ—¶æŠŠpç»‘ä¸Šæ¥
	mp.spinning = spinning
	mp.nextp.set(_p_)
	// å”¤é†’m
	notewakeup(&mp.park)
}
```

#### newm

newm é€šè¿‡allocmå‡½æ•°æ¥åˆ›å»ºæ–°m

```go
func newm(fn func(), _p_ *p) {
	// æ–°å»ºä¸€ä¸ªm
	mp := allocm(_p_, fn)
	// ä¸ºè¿™ä¸ªæ–°å»ºçš„mç»‘å®šæŒ‡å®šçš„p
	mp.nextp.set(_p_)
	// ... çœç•¥ ...
	// åˆ›å»ºç³»ç»Ÿçº¿ç¨‹
	newm1(mp)
}
```

#### new1m

```go
func newm1(mp *m) {
	// runtime cgoåŒ…ä¼šæŠŠiscgoè®¾ç½®ä¸ºtrueï¼Œè¿™é‡Œä¸åˆ†æ
	if iscgo {
		var ts cgothreadstart
		if _cgo_thread_start == nil {
			throw("_cgo_thread_start missing")
		}
		ts.g.set(mp.g0)
		ts.tls = (*uint64)(unsafe.Pointer(&mp.tls[0]))
		ts.fn = unsafe.Pointer(funcPC(mstart))
		if msanenabled {
			msanwrite(unsafe.Pointer(&ts), unsafe.Sizeof(ts))
		}
		execLock.rlock() // Prevent process clone.
		asmcgocall(_cgo_thread_start, unsafe.Pointer(&ts))
		execLock.runlock()
		return
	}
	execLock.rlock() // Prevent process clone.
	newosproc(mp)
	execLock.runlock()
}
```

#### newosproc

newosproc åˆ›å»ºä¸€ä¸ªæ–°çš„ç³»ç»Ÿçº¿ç¨‹ï¼Œå¹¶æ‰§è¡Œmstart_stubå‡½æ•°ï¼Œä¹‹åè°ƒç”¨`mstart`å‡½æ•°è¿›å…¥è°ƒåº¦ï¼Œåé¢åœ¨æ‰§è¡Œæµç¨‹ä¼šåˆ†æ

```go
func newosproc(mp *m) {
	stk := unsafe.Pointer(mp.g0.stack.hi)
	// Initialize an attribute object.
	var attr pthreadattr
	var err int32
	err = pthread_attr_init(&attr)

	// Finally, create the thread. It starts at mstart_stub, which does some low-level
	// setup and then calls mstart.
	var oset sigset
	sigprocmask(_SIG_SETMASK, &sigset_all, &oset)
	// åˆ›å»ºçº¿ç¨‹ï¼Œå¹¶ä¼ å…¥å¯åŠ¨å¯åŠ¨å‡½æ•° mstart_stubï¼Œ mstart_stub ä¹‹åè°ƒç”¨mstart
	err = pthread_create(&attr, funcPC(mstart_stub), unsafe.Pointer(mp))
	sigprocmask(_SIG_SETMASK, &oset, nil)
	if err != 0 {
		write(2, unsafe.Pointer(&failthreadcreate[0]), int32(len(failthreadcreate)))
		exit(1)
	}
}
```

#### allocm

allocmè¿™é‡Œé¦–å…ˆä¼šé‡Šæ”¾ schedçš„freemï¼Œç„¶åå†å»åˆ›å»ºmï¼Œå¹¶åˆå§‹åŒ–m

```go
func allocm(_p_ *p, fn func()) *m {
	_g_ := getg()
	_g_.m.locks++ // disable GC because it can be called from sysmon
	if _g_.m.p == 0 {
		acquirep(_p_) // temporarily borrow p for mallocs in this function
	}

	// Release the free M list. We need to do this somewhere and
	// this may free up a stack we can use.
	// é¦–å…ˆé‡Šæ”¾æ‰freemåˆ—è¡¨
	if sched.freem != nil {
		lock(&sched.lock)
		var newList *m
		for freem := sched.freem; freem != nil; {
			if freem.freeWait != 0 {
				next := freem.freelink
				freem.freelink = newList
				newList = freem
				freem = next
				continue
			}
			stackfree(freem.g0.stack)
			freem = freem.freelink
		}
		sched.freem = newList
		unlock(&sched.lock)
	}

	mp := new(m)
	// å¯åŠ¨å‡½æ•°ï¼Œæ ¹æ®startmè°ƒç”¨æ¥çœ‹ï¼Œè¿™ä¸ªfnå°±æ˜¯ mspinningï¼Œ ä¼šå°†m.mspinningè®¾ç½®ä¸ºtrue
	mp.mstartfn = fn
	// åˆå§‹åŒ–mï¼Œä¸Šé¢å·²ç»åˆ†æäº†
	mcommoninit(mp)
	// In case of cgo or Solaris or Darwin, pthread_create will make us a stack.
	// Windows and Plan 9 will layout sched stack on OS stack.
	// ä¸ºæ–°çš„måˆ›å»ºg0
	if iscgo || GOOS == "solaris" || GOOS == "windows" || GOOS == "plan9" || GOOS == "darwin" {
		mp.g0 = malg(-1)
	} else {
		mp.g0 = malg(8192 * sys.StackGuardMultiplier)
	}
	// ä¸ºmpçš„g0ç»‘å®šè‡ªå·±
	mp.g0.m = mp
	// å¦‚æœå½“å‰çš„mæ‰€ç»‘å®šçš„æ˜¯å‚æ•°ä¼ é€’è¿‡æ¥çš„pï¼Œè§£é™¤ç»‘å®šï¼Œå› ä¸ºå‚æ•°ä¼ é€’è¿‡æ¥çš„pç¨åè¦ç»‘å®šæ–°å»ºçš„m
	if _p_ == _g_.m.p.ptr() {
		releasep()
	}

	_g_.m.locks--
	if _g_.m.locks == 0 && _g_.preempt { // restore the preemption request in case we've cleared it in newstack
		_g_.stackguard0 = stackPreempt
	}

	return mp
}
```

#### notewakeup

```go
func notewakeup(n *note) {
	var v uintptr
	// è®¾ç½®m ä¸ºlocked
	for {
		v = atomic.Loaduintptr(&n.key)
		if atomic.Casuintptr(&n.key, v, locked) {
			break
		}
	}

	// Successfully set waitm to locked.
	// What was it before?
	// æ ¹æ®mçš„åŸå…ˆçš„çŠ¶æ€ï¼Œæ¥åˆ¤æ–­åé¢çš„æ‰§è¡Œæµç¨‹ï¼Œ0åˆ™ç›´æ¥è¿”å›ï¼Œlockedåˆ™å†²çªï¼Œå¦åˆ™è®¤ä¸ºæ˜¯watingï¼Œå”¤é†’
	switch {
	case v == 0:
		// Nothing was waiting. Done.
	case v == locked:
		// Two notewakeups! Not allowed.
		throw("notewakeup - double wakeup")
	default:
		// Must be the waiting m. Wake it up.
		// å”¤é†’ç³»ç»Ÿçº¿ç¨‹
		semawakeup((*m)(unsafe.Pointer(v)))
	}
}
```

è‡³æ­¤çš„è¯ï¼Œåˆ›å»ºå®Œä»»åŠ¡gåï¼Œå°†gæ”¾å…¥äº†pçš„localé˜Ÿåˆ—æˆ–è€…æ˜¯å…¨å±€é˜Ÿåˆ—ï¼Œç„¶åå¼€å§‹è·å–äº†ä¸€ä¸ªç©ºé—²çš„mæˆ–è€…æ–°å»ºä¸€ä¸ªmæ¥æ‰§è¡Œgï¼Œm, p, g éƒ½å·²ç»å‡†å¤‡å®Œæˆäº†ï¼Œä¸‹é¢å°±æ˜¯å¼€å§‹è°ƒåº¦ï¼Œæ¥è¿è¡Œä»»åŠ¡gäº†

## æ‰§è¡Œ

åœ¨startmå‡½æ•°åˆ†æçš„è¿‡ç¨‹ä¸­ä¼šï¼Œå¯ä»¥çœ‹åˆ°ï¼Œæœ‰ä¸¤ç§è·å–mçš„æ–¹å¼

- æ–°å»ºï¼š è¿™æ—¶å€™æ‰§è¡Œnewm1ä¸‹çš„newosprocï¼ŒåŒæ—¶æœ€ç»ˆè°ƒç”¨mstartæ¥æ‰§è¡Œè°ƒåº¦
- å”¤é†’ç©ºé—²mï¼šä»ä¼‘çœ çš„åœ°æ–¹ç»§ç»­æ‰§è¡Œ

mæ‰§è¡Œgæœ‰ä¸¤ä¸ªèµ·ç‚¹ï¼Œä¸€ä¸ªæ˜¯çº¿ç¨‹å¯åŠ¨å‡½æ•° `mstart`ï¼Œ å¦ä¸€ä¸ªåˆ™æ˜¯ä¼‘çœ è¢«å”¤é†’åçš„è°ƒåº¦`schedule`äº†ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹ï¼Œä¹Ÿå°±æ˜¯`mstart`ï¼Œ `mstart` èµ°åˆ°æœ€åä¹Ÿæ˜¯ `schedule` è°ƒåº¦

### mstart

```go
func mstart() {
	_g_ := getg()

	osStack := _g_.stack.lo == 0
	if osStack {
		// Initialize stack bounds from system stack.
		// Cgo may have left stack size in stack.hi.
		// minit may update the stack bounds.
		// ä»ç³»ç»Ÿå †æ ˆä¸Šç›´æ¥åˆ’å‡ºæ‰€éœ€çš„èŒƒå›´
		size := _g_.stack.hi
		if size == 0 {
			size = 8192 * sys.StackGuardMultiplier
		}
		_g_.stack.hi = uintptr(noescape(unsafe.Pointer(&size)))
		_g_.stack.lo = _g_.stack.hi - size + 1024
	}
	// Initialize stack guards so that we can start calling
	// both Go and C functions with stack growth prologues.
	_g_.stackguard0 = _g_.stack.lo + _StackGuard
	_g_.stackguard1 = _g_.stackguard0
	// è°ƒç”¨mstart1æ¥å¤„ç†
	mstart1()

	// Exit this thread.
	if GOOS == "windows" || GOOS == "solaris" || GOOS == "plan9" || GOOS == "darwin" {
		// Window, Solaris, Darwin and Plan 9 always system-allocate
		// the stack, but put it in _g_.stack before mstart,
		// so the logic above hasn't set osStack yet.
		osStack = true
	}
	// é€€å‡ºmï¼Œæ­£å¸¸æƒ…å†µä¸‹mstart1è°ƒç”¨schedule() æ—¶ï¼Œæ˜¯ä¸å†è¿”å›çš„ï¼Œæ‰€ä»¥ï¼Œä¸ç”¨æ‹…å¿ƒç³»ç»Ÿçº¿ç¨‹çš„é¢‘ç¹åˆ›å»ºé€€å‡º
	mexit(osStack)
}
```

### mstart1

```go
func mstart1() {
	_g_ := getg()

	if _g_ != _g_.m.g0 {
		throw("bad runtimeÂ·mstart")
	}

	// Record the caller for use as the top of stack in mcall and
	// for terminating the thread.
	// We're never coming back to mstart1 after we call schedule,
	// so other calls can reuse the current frame.
	// ä¿å­˜è°ƒç”¨è€…çš„pc spç­‰ä¿¡æ¯
	save(getcallerpc(), getcallersp())
	asminit()
	// åˆå§‹åŒ–mçš„sigalçš„æ ˆå’Œmask
	minit()

	// Install signal handlers; after minit so that minit can
	// prepare the thread to be able to handle the signals.
	// å®‰è£…sigalå¤„ç†å™¨
	if _g_.m == &m0 {
		mstartm0()
	}
	// å¦‚æœè®¾ç½®äº†mstartfnï¼Œå°±å…ˆæ‰§è¡Œè¿™ä¸ª
	if fn := _g_.m.mstartfn; fn != nil {
		fn()
	}

	if _g_.m.helpgc != 0 {
		_g_.m.helpgc = 0
		stopm()
	} else if _g_.m != &m0 {
		// è·å–nextp
		acquirep(_g_.m.nextp.ptr())
		_g_.m.nextp = 0
	}
	schedule()
}
```

#### acquirep

acquirep å‡½æ•°ä¸»è¦æ˜¯æ”¹å˜pçš„çŠ¶æ€ï¼Œç»‘å®š m pï¼Œé€šè¿‡å§pçš„mcacheä¸må…±äº«

```go
func acquirep(_p_ *p) {
	// Do the part that isn't allowed to have write barriers.
	acquirep1(_p_)

	// have p; write barriers now allowed
	_g_ := getg()
	// æŠŠpçš„mcacheä¸må…±äº«
	_g_.m.mcache = _p_.mcache
}
```

#### acquirep1

```go
func acquirep1(_p_ *p) {
	_g_ := getg()

	// è®©m päº’ç›¸ç»‘å®š
	_g_.m.p.set(_p_)
	_p_.m.set(_g_.m)
	_p_.status = _Prunning
}
```

#### schedule

å¼€å§‹è¿›å…¥åˆ°è°ƒåº¦å‡½æ•°äº†ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±scheduleã€executeã€goroutine fnã€goexitæ„æˆçš„é€»è¾‘å¾ªç¯ï¼Œå°±ç®—mæ˜¯å”¤é†’åï¼Œä¹Ÿæ˜¯ä»è®¾ç½®çš„æ–­ç‚¹å¼€å§‹æ‰§è¡Œ

```go
func schedule() {
	_g_ := getg()

	if _g_.m.locks != 0 {
		throw("schedule: holding locks")
	}
	// å¦‚æœæœ‰lockgï¼Œåœæ­¢æ‰§è¡Œå½“å‰çš„m
	if _g_.m.lockedg != 0 {
		// è§£é™¤lockedmçš„é”å®šï¼Œå¹¶æ‰§è¡Œå½“å‰g
		stoplockedm()
		execute(_g_.m.lockedg.ptr(), false) // Never returns.
	}

	// We should not schedule away from a g that is executing a cgo call,
	// since the cgo call is using the m's g0 stack.
	if _g_.m.incgo {
		throw("schedule: in cgo")
	}

top:
	// gc ç­‰å¾…
	if sched.gcwaiting != 0 {
		gcstopm()
		goto top
	}

	var gp *g
	var inheritTime bool

	if gp == nil {
		// Check the global runnable queue once in a while to ensure fairness.
		// Otherwise two goroutines can completely occupy the local runqueue
		// by constantly respawning each other.
		// ä¸ºäº†ä¿è¯å…¬å¹³ï¼Œæ¯éš”61æ¬¡ï¼Œä»å…¨å±€é˜Ÿåˆ—ä¸Šè·å–g
		if _g_.m.p.ptr().schedtick%61 == 0 && sched.runqsize > 0 {
			lock(&sched.lock)
			gp = globrunqget(_g_.m.p.ptr(), 1)
			unlock(&sched.lock)
		}
	}
	if gp == nil {
		// å…¨å±€é˜Ÿåˆ—ä¸Šè·å–ä¸åˆ°å¾…è¿è¡Œçš„gï¼Œåˆ™ä»p localé˜Ÿåˆ—ä¸­è·å–
		gp, inheritTime = runqget(_g_.m.p.ptr())
		if gp != nil && _g_.m.spinning {
			throw("schedule: spinning with local work")
		}
	}
	if gp == nil {
		// å¦‚æœp localè·å–ä¸åˆ°å¾…è¿è¡Œgï¼Œåˆ™å¼€å§‹æŸ¥æ‰¾ï¼Œè¿™ä¸ªå‡½æ•°ä¼šä» å…¨å±€ io pollï¼Œ p loclå’Œå…¶ä»–p localè·å–å¾…è¿è¡Œçš„gï¼Œåé¢è¯¦ç»†åˆ†æ
		gp, inheritTime = findrunnable() // blocks until work is available
	}

	// This thread is going to run a goroutine and is not spinning anymore,
	// so if it was marked as spinning we need to reset it now and potentially
	// start a new spinning M.
	if _g_.m.spinning {
		// å¦‚æœmæ˜¯è‡ªæ—‹çŠ¶æ€ï¼Œå–æ¶ˆè‡ªæ—‹
		resetspinning()
	}

	if gp.lockedm != 0 {
		// Hands off own p to the locked m,
		// then blocks waiting for a new p.
		// å¦‚æœgæœ‰lockedmï¼Œåˆ™ä¼‘çœ ä¸Šäº¤pï¼Œä¼‘çœ mï¼Œç­‰å¾…æ–°çš„mï¼Œå”¤é†’åä»è¿™é‡Œå¼€å§‹æ‰§è¡Œï¼Œè·³è½¬åˆ°top
		startlockedm(gp)
		goto top
	}
	// å¼€å§‹æ‰§è¡Œè¿™ä¸ªg
	execute(gp, inheritTime)
}
```

##### stoplockedm

å› ä¸ºå½“å‰çš„mç»‘å®šäº†lockedgï¼Œè€Œå½“å‰gä¸æ˜¯æŒ‡å®šçš„lockedgï¼Œæ‰€ä»¥è¿™ä¸ªmä¸èƒ½æ‰§è¡Œï¼Œä¸Šäº¤å½“å‰mç»‘å®šçš„pï¼Œå¹¶ä¸”ä¼‘çœ mç›´åˆ°è°ƒåº¦lockedg

```go
func stoplockedm() {
	_g_ := getg()

	if _g_.m.lockedg == 0 || _g_.m.lockedg.ptr().lockedm.ptr() != _g_.m {
		throw("stoplockedm: inconsistent locking")
	}
	if _g_.m.p != 0 {
		// Schedule another M to run this p.
		// é‡Šæ”¾å½“å‰p
		_p_ := releasep()
		handoffp(_p_)
	}
	incidlelocked(1)
	// Wait until another thread schedules lockedg again.
	notesleep(&_g_.m.park)
	noteclear(&_g_.m.park)
	status := readgstatus(_g_.m.lockedg.ptr())
	if status&^_Gscan != _Grunnable {
		print("runtime:stoplockedm: g is not Grunnable or Gscanrunnable\n")
		dumpgstatus(_g_)
		throw("stoplockedm: not runnable")
	}
	// ä¸Šäº¤äº†å½“å‰çš„pï¼Œå°†nextpè®¾ç½®ä¸ºå¯æ‰§è¡Œçš„p
	acquirep(_g_.m.nextp.ptr())
	_g_.m.nextp = 0
}
```

##### startlockedm

è°ƒåº¦ lockedmå»è¿è¡Œlockedg

```GO
func startlockedm(gp *g) {
	_g_ := getg()

	mp := gp.lockedm.ptr()
	if mp == _g_.m {
		throw("startlockedm: locked to me")
	}
	if mp.nextp != 0 {
		throw("startlockedm: m has p")
	}
	// directly handoff current P to the locked m
	incidlelocked(-1)
	// ç§»äº¤å½“å‰pç»™lockedmï¼Œå¹¶è®¾ç½®ä¸ºlockedm.nextpï¼Œä»¥ä¾¿äºlockedmå”¤é†’åï¼Œå¯ä»¥è·å–
	_p_ := releasep()
	mp.nextp.set(_p_)
	// mè¢«å”¤é†’åï¼Œä»mä¼‘çœ çš„åœ°æ–¹å¼€å§‹æ‰§è¡Œï¼Œä¹Ÿå°±æ˜¯schedule()å‡½æ•°ä¸­
	notewakeup(&mp.park)
	stopm()
}
```

##### handoffp

```go
func handoffp(_p_ *p) {
	// handoffp must start an M in any situation where
	// findrunnable would return a G to run on _p_.

	// if it has local work, start it straight away
	if !runqempty(_p_) || sched.runqsize != 0 {
		// è°ƒç”¨startmå¼€å§‹è°ƒåº¦
		startm(_p_, false)
		return
	}

	// no local work, check that there are no spinning/idle M's,
	// otherwise our help is not required
	// åˆ¤æ–­æœ‰æ²¡æœ‰æ­£åœ¨å¯»æ‰¾pçš„mä»¥åŠæœ‰æ²¡æœ‰ç©ºé—²çš„p
	if atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) == 0 && atomic.Cas(&sched.nmspinning, 0, 1) { // TODO: fast atomic
		startm(_p_, true)
		return
	}
	lock(&sched.lock)

	if _p_.runSafePointFn != 0 && atomic.Cas(&_p_.runSafePointFn, 1, 0) {
		sched.safePointFn(_p_)
		sched.safePointWait--
		if sched.safePointWait == 0 {
			notewakeup(&sched.safePointNote)
		}
	}
	// å¦‚æœ å…¨å±€å¾…è¿è¡Œgé˜Ÿåˆ—ä¸ä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨startmè¿›è¡Œè°ƒåº¦
	if sched.runqsize != 0 {
		unlock(&sched.lock)
		startm(_p_, false)
		return
	}
	// If this is the last running P and nobody is polling network,
	// need to wakeup another M to poll network.
	if sched.npidle == uint32(gomaxprocs-1) && atomic.Load64(&sched.lastpoll) != 0 {
		unlock(&sched.lock)
		startm(_p_, false)
		return
	}
	// æŠŠpæ”¾å…¥åˆ°å…¨å±€çš„ç©ºé—²é˜Ÿåˆ—ï¼Œæ”¾å›é˜Ÿåˆ—å°±ä¸å¤šè¯´äº†ï¼Œå‚è€ƒallmçš„æ”¾å›
	pidleput(_p_)
	unlock(&sched.lock)
}
```

##### execute

å¼€å§‹æ‰§è¡Œgçš„ä»£ç äº†

```go
func execute(gp *g, inheritTime bool) {
	_g_ := getg()
	// æ›´æ”¹gçš„çŠ¶æ€ï¼Œå¹¶ä¸å…è®¸æŠ¢å 
	casgstatus(gp, _Grunnable, _Grunning)
	gp.waitsince = 0
	gp.preempt = false
	gp.stackguard0 = gp.stack.lo + _StackGuard
	if !inheritTime {
		// è°ƒåº¦è®¡æ•°
		_g_.m.p.ptr().schedtick++
	}
	_g_.m.curg = gp
	gp.m = _g_.m
	// å¼€å§‹æ‰§è¡Œgçš„ä»£ç äº†
	gogo(&gp.sched)
}
```

##### gogo

gogoå‡½æ•°æ‰¿è½½çš„ä½œç”¨å°±æ˜¯åˆ‡æ¢åˆ°gçš„æ ˆï¼Œå¼€å§‹æ‰§è¡Œgçš„ä»£ç ï¼Œæ±‡ç¼–å†…å®¹å°±ä¸åˆ†æäº†ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªç–‘é—®å°±æ˜¯ï¼Œgogoæ‰§è¡Œå®Œå‡½æ•°åï¼Œæ€ä¹ˆå†æ¬¡è¿›å…¥è°ƒåº¦å‘¢ï¼Ÿ

æˆ‘ä»¬å›åˆ°`newproc1`å‡½æ•°çš„L63 `newg.sched.pc = funcPC(goexit) + sys.PCQuantum` ï¼Œè¿™é‡Œä¿å­˜äº†pcçš„è´¨åœ°ä¸ºgoexitçš„åœ°å€ï¼Œæ‰€ä»¥å½“æ‰§è¡Œå®Œç”¨æˆ·ä»£ç åï¼Œå°±ä¼šè¿›å…¥ `goexit` å‡½æ•°

##### goexit0

goexit åœ¨æ±‡ç¼–å±‚é¢å°±æ˜¯è°ƒç”¨ `runtime.goexit1`ï¼Œè€Œgoexit1é€šè¿‡ mcall è°ƒç”¨äº†`goexit0` æ‰€ä»¥è¿™é‡Œç›´æ¥åˆ†æäº†`goexit0`

`goexit0` é‡ç½®gçš„çŠ¶æ€ï¼Œå¹¶é‡æ–°è¿›è¡Œè°ƒåº¦ï¼Œè¿™æ ·å°±è°ƒåº¦å°±åˆå›åˆ°äº†`schedule()` äº†ï¼Œå¼€å§‹å¾ªç¯å¾€å¤çš„è°ƒåº¦

```go
func goexit0(gp *g) {
	_g_ := getg()
	// è½¬æ¢gçš„çŠ¶æ€ä¸ºdeadï¼Œä»¥æ”¾å›ç©ºé—²åˆ—è¡¨
	casgstatus(gp, _Grunning, _Gdead)
	if isSystemGoroutine(gp) {
		atomic.Xadd(&sched.ngsys, -1)
	}
	// æ¸…ç©ºgçš„çŠ¶æ€
	gp.m = nil
	locked := gp.lockedm != 0
	gp.lockedm = 0
	_g_.m.lockedg = 0
	gp.paniconfault = false
	gp._defer = nil // should be true already but just in case.
	gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data.
	gp.writebuf = nil
	gp.waitreason = 0
	gp.param = nil
	gp.labels = nil
	gp.timer = nil

	// Note that gp's stack scan is now "valid" because it has no
	// stack.
	gp.gcscanvalid = true
	dropg()

	// æŠŠgæ”¾å›ç©ºé—²åˆ—è¡¨ï¼Œä»¥å¤‡å¤ç”¨
	gfput(_g_.m.p.ptr(), gp)
	// å†æ¬¡è¿›å…¥è°ƒåº¦å¾ªç¯
	schedule()
}
```

è‡³æ­¤ï¼Œå•æ¬¡è°ƒåº¦ç»“æŸï¼Œå†æ¬¡è¿›å…¥è°ƒåº¦ï¼Œå¾ªç¯å¾€å¤

##### findrunnable

```go
func findrunnable() (gp *g, inheritTime bool) {
	_g_ := getg()

	// The conditions here and in handoffp must agree: if
	// findrunnable would return a G to run, handoffp must start
	// an M.

top:
	_p_ := _g_.m.p.ptr()

	// local runq
	// ä»p local å»è·å–g
	if gp, inheritTime := runqget(_p_); gp != nil {
		return gp, inheritTime
	}

	// global runq
	// ä»å…¨å±€çš„å¾…è¿è¡Œdé˜Ÿåˆ—è·å–
	if sched.runqsize != 0 {
		lock(&sched.lock)
		gp := globrunqget(_p_, 0)
		unlock(&sched.lock)
		if gp != nil {
			return gp, false
		}
	}

	// Poll network.
	// This netpoll is only an optimization before we resort to stealing.
	// We can safely skip it if there are no waiters or a thread is blocked
	// in netpoll already. If there is any kind of logical race with that
	// blocked thread (e.g. it has already returned from netpoll, but does
	// not set lastpoll yet), this thread will do blocking netpoll below
	// anyway.
	// çœ‹çœ‹netpollä¸­æœ‰æ²¡æœ‰å·²ç»å‡†å¤‡å¥½çš„g
	if netpollinited() && atomic.Load(&netpollWaiters) > 0 && atomic.Load64(&sched.lastpoll) != 0 {
		if gp := netpoll(false); gp != nil { // non-blocking
			// netpoll returns list of goroutines linked by schedlink.
			injectglist(gp.schedlink.ptr())
			casgstatus(gp, _Gwaiting, _Grunnable)
			if trace.enabled {
				traceGoUnpark(gp, 0)
			}
			return gp, false
		}
	}

	// Steal work from other P's.
	// å¦‚æœsched.pidle == procs - 1ï¼Œè¯´æ˜æ‰€æœ‰çš„péƒ½æ˜¯ç©ºé—²çš„ï¼Œæ— éœ€éå†å…¶ä»–päº†
	procs := uint32(gomaxprocs)
	if atomic.Load(&sched.npidle) == procs-1 {
		// Either GOMAXPROCS=1 or everybody, except for us, is idle already.
		// New work can appear from returning syscall/cgocall, network or timers.
		// Neither of that submits to local run queues, so no point in stealing.
		goto stop
	}
	// If number of spinning M's >= number of busy P's, block.
	// This is necessary to prevent excessive CPU consumption
	// when GOMAXPROCS>>1 but the program parallelism is low.
	// å¦‚æœå¯»æ‰¾pçš„mçš„æ•°é‡ï¼Œå¤§äºæœ‰gçš„pçš„æ•°é‡çš„ä¸€èˆ¬ï¼Œå°±ä¸å†å»å¯»æ‰¾äº†
	if !_g_.m.spinning && 2*atomic.Load(&sched.nmspinning) >= procs-atomic.Load(&sched.npidle) {
		goto stop
	}
	// è®¾ç½®å½“å‰mçš„è‡ªæ—‹çŠ¶æ€
	if !_g_.m.spinning {
		_g_.m.spinning = true
		atomic.Xadd(&sched.nmspinning, 1)
	}
	// å¼€å§‹çªƒå–å…¶ä»–pçš„å¾…è¿è¡Œgäº†
	for i := 0; i < 4; i++ {
		for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() {
			if sched.gcwaiting != 0 {
				goto top
			}
			stealRunNextG := i > 2 // first look for ready queues with more than 1 g
			// ä»å…¶ä»–çš„på·å–ä¸€èˆ¬çš„ä»»åŠ¡æ•°é‡ï¼Œè¿˜ä¼šéšæœºå·å–pçš„runnextï¼ˆè¿‡åˆ†äº†ï¼‰ï¼Œå·å–éƒ¨åˆ†å°±ä¸åˆ†æäº†ï¼Œå°±æ˜¯sliceçš„æ“ä½œè€Œå·²
			if gp := runqsteal(_p_, allp[enum.position()], stealRunNextG); gp != nil {
				return gp, false
			}
		}
	}

stop:
	// å¯¹allåšä¸ªé•œåƒå¤‡ä»½
	allpSnapshot := allp

	// return P and block
	lock(&sched.lock)

	if sched.runqsize != 0 {
		gp := globrunqget(_p_, 0)
		unlock(&sched.lock)
		return gp, false
	}
	if releasep() != _p_ {
		throw("findrunnable: wrong p")
	}
	pidleput(_p_)
	unlock(&sched.lock)

	wasSpinning := _g_.m.spinning
	if _g_.m.spinning {
		// è®¾ç½®éè‡ªæ—‹çŠ¶æ€ï¼Œå› ä¸ºæ‰¾pçš„å·¥ä½œå·²ç»ç»“æŸäº†
		_g_.m.spinning = false
		if int32(atomic.Xadd(&sched.nmspinning, -1)) < 0 {
			throw("findrunnable: negative nmspinning")
		}
	}

	// check all runqueues once again
	for _, _p_ := range allpSnapshot {
		if !runqempty(_p_) {
			lock(&sched.lock)
			_p_ = pidleget()
			unlock(&sched.lock)
			if _p_ != nil {
				acquirep(_p_)
				if wasSpinning {
					_g_.m.spinning = true
					atomic.Xadd(&sched.nmspinning, 1)
				}
				goto top
			}
			break
		}
	}
	// poll network
	if netpollinited() && atomic.Load(&netpollWaiters) > 0 && atomic.Xchg64(&sched.lastpoll, 0) != 0 {
		if _g_.m.p != 0 {
			throw("findrunnable: netpoll with p")
		}
		if _g_.m.spinning {
			throw("findrunnable: netpoll with spinning")
		}
		gp := netpoll(true) // block until new work is available
		atomic.Store64(&sched.lastpoll, uint64(nanotime()))
		if gp != nil {
			lock(&sched.lock)
			_p_ = pidleget()
			unlock(&sched.lock)
			if _p_ != nil {
				acquirep(_p_)
				injectglist(gp.schedlink.ptr())
				casgstatus(gp, _Gwaiting, _Grunnable)
				if trace.enabled {
					traceGoUnpark(gp, 0)
				}
				return gp, false
			}
			injectglist(gp)
		}
	}
	stopm()
	goto top
}
```

è¿™é‡ŒçœŸçš„æ˜¯æ— å¥ˆå•Šï¼Œä¸ºäº†å¯»æ‰¾ä¸€ä¸ªå¯è¿è¡Œçš„gï¼Œä¹Ÿæ˜¯ç…è´¹è‹¦å¿ƒï¼ŒåŠæ—¶è¿›å…¥äº†stop çš„labelï¼Œè¿˜æ˜¯ä¸æ­»å¿ƒï¼Œåˆæ¥äº†ä¸€è¾¹å¯»æ‰¾ã€‚å¤§è‡´å¯»æ‰¾è¿‡ç¨‹å¯ä»¥æ€»ç»“ä¸ºä¸€ä¸‹å‡ ä¸ªï¼š

- ä»pè‡ªå·±çš„localé˜Ÿåˆ—ä¸­è·å–å¯è¿è¡Œçš„g
- ä»å…¨å±€é˜Ÿåˆ—ä¸­è·å–å¯è¿è¡Œçš„g
- ä»netpollä¸­è·å–ä¸€ä¸ªå·²ç»å‡†å¤‡å¥½çš„g
- ä»å…¶ä»–pçš„localé˜Ÿåˆ—ä¸­è·å–å¯è¿è¡Œçš„gï¼Œéšæœºå·å–pçš„runnextï¼Œæœ‰ç‚¹ä»»æ€§
- æ— è®ºå¦‚ä½•éƒ½è·å–ä¸åˆ°çš„è¯ï¼Œå°±stopmäº†

##### stopm

stopä¼šæŠŠå½“å‰mæ”¾åˆ°ç©ºé—²åˆ—è¡¨é‡Œé¢ï¼ŒåŒæ—¶ç»‘å®šm.nextp ä¸ m

```go
func stopm() {
	_g_ := getg()
retry:
	lock(&sched.lock)
	// æŠŠå½“å‰mæ”¾åˆ°sched.midle çš„ç©ºé—²åˆ—è¡¨é‡Œ
	mput(_g_.m)
	unlock(&sched.lock)
	// ä¼‘çœ ï¼Œç­‰å¾…è¢«å”¤é†’
	notesleep(&_g_.m.park)
	noteclear(&_g_.m.park)
	// ç»‘å®šp
	acquirep(_g_.m.nextp.ptr())
	_g_.m.nextp = 0
}
```

## ç›‘æ§

### sysmon

goçš„ç›‘æ§æ˜¯ä¾é å‡½æ•° sysmon æ¥å®Œæˆçš„ï¼Œç›‘æ§ä¸»è¦åšä¸€ä¸‹å‡ ä»¶äº‹

- é‡Šæ”¾é—²ç½®è¶…è¿‡5åˆ†é’Ÿçš„spanç‰©ç†å†…å­˜
- å¦‚æœè¶…è¿‡ä¸¤åˆ†é’Ÿæ²¡æœ‰æ‰§è¡Œåƒåœ¾å›æ”¶ï¼Œåˆ™å¼ºåˆ¶æ‰§è¡Œ
- å°†é•¿æ—¶é—´æœªå¤„ç†çš„netpollç»“æœæ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
- å‘é•¿æ—¶é—´è¿è¡Œçš„gè¿›è¡ŒæŠ¢å 
- æ”¶å›å› ä¸ºsyscallè€Œé•¿æ—¶é—´é˜»å¡çš„p

ç›‘æ§çº¿ç¨‹å¹¶ä¸æ˜¯æ—¶åˆ»åœ¨è¿è¡Œçš„ï¼Œç›‘æ§çº¿ç¨‹é¦–æ¬¡ä¼‘çœ 20usï¼Œæ¯æ¬¡æ‰§è¡Œå®Œåï¼Œå¢åŠ ä¸€å€çš„ä¼‘çœ æ—¶é—´ï¼Œä½†æ˜¯æœ€å¤šä¼‘çœ 10ms

```go
func sysmon() {
	lock(&sched.lock)
	sched.nmsys++
	checkdead()
	unlock(&sched.lock)

	// If a heap span goes unused for 5 minutes after a garbage collection,
	// we hand it back to the operating system.
	scavengelimit := int64(5 * 60 * 1e9)

	if debug.scavenge > 0 {
		// Scavenge-a-lot for testing.
		forcegcperiod = 10 * 1e6
		scavengelimit = 20 * 1e6
	}

	lastscavenge := nanotime()
	nscavenge := 0

	lasttrace := int64(0)
	idle := 0 // how many cycles in succession we had not wokeup somebody
	delay := uint32(0)
	for {
		// åˆ¤æ–­å½“å‰å¾ªç¯ï¼Œåº”è¯¥ä¼‘çœ çš„æ—¶é—´
		if idle == 0 { // start with 20us sleep...
			delay = 20
		} else if idle > 50 { // start doubling the sleep after 1ms...
			delay *= 2
		}
		if delay > 10*1000 { // up to 10ms
			delay = 10 * 1000
		}
		usleep(delay)
		// STWæ—¶ä¼‘çœ sysmon
		if debug.schedtrace <= 0 && (sched.gcwaiting != 0 || atomic.Load(&sched.npidle) == uint32(gomaxprocs)) {
			lock(&sched.lock)
			if atomic.Load(&sched.gcwaiting) != 0 || atomic.Load(&sched.npidle) == uint32(gomaxprocs) {
				atomic.Store(&sched.sysmonwait, 1)
				unlock(&sched.lock)
				// Make wake-up period small enough
				// for the sampling to be correct.
				maxsleep := forcegcperiod / 2
				if scavengelimit < forcegcperiod {
					maxsleep = scavengelimit / 2
				}
				shouldRelax := true
				if osRelaxMinNS > 0 {
					next := timeSleepUntil()
					now := nanotime()
					if next-now < osRelaxMinNS {
						shouldRelax = false
					}
				}
				if shouldRelax {
					osRelax(true)
				}
				// è¿›è¡Œä¼‘çœ 
				notetsleep(&sched.sysmonnote, maxsleep)
				if shouldRelax {
					osRelax(false)
				}
				lock(&sched.lock)
				// å”¤é†’åï¼Œæ¸…é™¤ä¼‘çœ çŠ¶æ€ï¼Œç»§ç»­æ‰§è¡Œ
				atomic.Store(&sched.sysmonwait, 0)
				noteclear(&sched.sysmonnote)
				idle = 0
				delay = 20
			}
			unlock(&sched.lock)
		}
		// trigger libc interceptors if needed
		if *cgo_yield != nil {
			asmcgocall(*cgo_yield, nil)
		}
		// poll network if not polled for more than 10ms
		lastpoll := int64(atomic.Load64(&sched.lastpoll))
		now := nanotime()
		// å¦‚æœnetpollä¸ä¸ºç©ºï¼Œæ¯éš”10msæ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰okçš„
		if netpollinited() && lastpoll != 0 && lastpoll+10*1000*1000 < now {
			atomic.Cas64(&sched.lastpoll, uint64(lastpoll), uint64(now))
			// è¿”å›äº†å·²ç»è·å–åˆ°ç»“æœçš„goroutineçš„åˆ—è¡¨
			gp := netpoll(false) // non-blocking - returns list of goroutines
			if gp != nil {
				incidlelocked(-1)
				// æŠŠè·å–åˆ°çš„gçš„åˆ—è¡¨åŠ å…¥åˆ°å…¨å±€å¾…è¿è¡Œé˜Ÿåˆ—ä¸­
				injectglist(gp)
				incidlelocked(1)
			}
		}
		// retake P's blocked in syscalls
		// and preempt long running G's
		// æŠ¢å¤ºsyscallé•¿æ—¶é—´é˜»å¡çš„på’Œé•¿æ—¶é—´è¿è¡Œçš„g
		if retake(now) != 0 {
			idle = 0
		} else {
			idle++
		}
		// check if we need to force a GC
		// é€šè¿‡gcTrigger.test() å‡½æ•°åˆ¤æ–­æ˜¯å¦è¶…è¿‡è®¾å®šçš„å¼ºåˆ¶è§¦å‘gcçš„æ—¶é—´é—´éš”ï¼Œ
		if t := (gcTrigger{kind: gcTriggerTime, now: now}); t.test() && atomic.Load(&forcegc.idle) != 0 {
			lock(&forcegc.lock)
			forcegc.idle = 0
			forcegc.g.schedlink = 0
			// æŠŠgcçš„gåŠ å…¥å¾…è¿è¡Œé˜Ÿåˆ—ï¼Œç­‰å¾…è°ƒåº¦è¿è¡Œ
			injectglist(forcegc.g)
			unlock(&forcegc.lock)
		}
		// scavenge heap once in a while
		// åˆ¤æ–­æ˜¯å¦æœ‰5åˆ†é’Ÿæœªä½¿ç”¨çš„spanï¼Œæœ‰çš„è¯ï¼Œå½’è¿˜ç»™ç³»ç»Ÿ
		if lastscavenge+scavengelimit/2 < now {
			mheap_.scavenge(int32(nscavenge), uint64(now), uint64(scavengelimit))
			lastscavenge = now
			nscavenge++
		}
		if debug.schedtrace > 0 && lasttrace+int64(debug.schedtrace)*1000000 <= now {
			lasttrace = now
			schedtrace(debug.scheddetail > 0)
		}
	}
}
```

æ‰«ænetpollï¼Œå¹¶æŠŠgå­˜æ”¾åˆ°å»å…¨å±€é˜Ÿåˆ—æ¯”è¾ƒå¥½ç†è§£ï¼Œè·Ÿå‰é¢æ·»åŠ på’Œmçš„é€»è¾‘å·®ä¸å¤šï¼Œä½†æ˜¯æŠ¢å è¿™é‡Œå°±ä¸æ˜¯å¾ˆç†è§£äº†ï¼Œä½ è¯´æŠ¢å å°±æŠ¢å ï¼Œè¢«æŠ¢å çš„gå²‚ä¸æ˜¯å¾ˆæ²¡é¢å­ï¼Œè€Œä¸”æ€ä¹ˆæŠ¢å å‘¢ï¼Ÿ

### retake

```go
const forcePreemptNS = 10 * 1000 * 1000 // 10ms

func retake(now int64) uint32 {
	n := 0
	// Prevent allp slice changes. This lock will be completely
	// uncontended unless we're already stopping the world.
	lock(&allpLock)
	// We can't use a range loop over allp because we may
	// temporarily drop the allpLock. Hence, we need to re-fetch
	// allp each time around the loop.
	for i := 0; i < len(allp); i++ {
		_p_ := allp[i]
		if _p_ == nil {
			// This can happen if procresize has grown
			// allp but not yet created new Ps.
			continue
		}
		pd := &_p_.sysmontick
		s := _p_.status
		if s == _Psyscall {
			// Retake P from syscall if it's there for more than 1 sysmon tick (at least 20us).
			// pd.syscalltick å³ _p_.sysmontick.syscalltick åªæœ‰åœ¨sysmonçš„æ—¶å€™ä¼šæ›´æ–°ï¼Œè€Œ _p_.syscalltick åˆ™ä¼šæ¯æ¬¡éƒ½æ›´æ–°ï¼Œæ‰€ä»¥ï¼Œå½“syscallä¹‹åï¼Œç¬¬ä¸€ä¸ªsysmonæ£€æµ‹åˆ°çš„æ—¶å€™å¹¶ä¸ä¼šæŠ¢å ï¼Œè€Œæ˜¯ç¬¬äºŒæ¬¡å¼€å§‹æ‰ä¼šæŠ¢å ï¼Œä¸­é—´é—´éš”è‡³å°‘æœ‰20usï¼Œæœ€å¤šä¼šæœ‰10ms
			t := int64(_p_.syscalltick)
			if int64(pd.syscalltick) != t {
				pd.syscalltick = uint32(t)
				pd.syscallwhen = now
				continue
			}
			// On the one hand we don't want to retake Ps if there is no other work to do,
			// but on the other hand we want to retake them eventually
			// because they can prevent the sysmon thread from deep sleep.
			// æ˜¯å¦æœ‰ç©ºpï¼Œæœ‰å¯»æ‰¾pçš„mï¼Œä»¥åŠå½“å‰çš„påœ¨syscallä¹‹åï¼Œæœ‰æ²¡æœ‰è¶…è¿‡10ms
			if runqempty(_p_) && atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) > 0 && pd.syscallwhen+10*1000*1000 > now {
				continue
			}
			// Drop allpLock so we can take sched.lock.
			unlock(&allpLock)
			// Need to decrement number of idle locked M's
			// (pretending that one more is running) before the CAS.
			// Otherwise the M from which we retake can exit the syscall,
			// increment nmidle and report deadlock.
			incidlelocked(-1)
			// æŠ¢å pï¼ŒæŠŠpçš„çŠ¶æ€è½¬ä¸ºidleçŠ¶æ€
			if atomic.Cas(&_p_.status, s, _Pidle) {
				if trace.enabled {
					traceGoSysBlock(_p_)
					traceProcStop(_p_)
				}
				n++
				_p_.syscalltick++
				// æŠŠå½“å‰pç§»äº¤å‡ºå»ï¼Œä¸Šé¢å·²ç»åˆ†æè¿‡äº†
				handoffp(_p_)
			}
			incidlelocked(1)
			lock(&allpLock)
		} else if s == _Prunning {
			// Preempt G if it's running for too long.
			// å¦‚æœpæ˜¯runningçŠ¶æ€ï¼Œå¦‚æœpä¸‹é¢çš„gæ‰§è¡Œå¤ªä¹…äº†ï¼Œåˆ™æŠ¢å 
			t := int64(_p_.schedtick)
			if int64(pd.schedtick) != t {
				pd.schedtick = uint32(t)
				pd.schedwhen = now
				continue
			}
			// åˆ¤æ–­æ˜¯å¦è¶…å‡º10ms, ä¸è¶…è¿‡ä¸æŠ¢å 
			if pd.schedwhen+forcePreemptNS > now {
				continue
			}
			// å¼€å§‹æŠ¢å 
			preemptone(_p_)
		}
	}
	unlock(&allpLock)
	return uint32(n)
}
```

### preemptone

è¿™ä¸ªå‡½æ•°çš„æ³¨é‡Šï¼Œä½œè€…å°±è¡¨æ˜è¿™ç§æŠ¢å å¹¶ä¸æ˜¯å¾ˆé è°±ğŸ˜‚ï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹å®ç°å§

```go
func preemptone(_p_ *p) bool {
	mp := _p_.m.ptr()
	if mp == nil || mp == getg().m {
		return false
	}
	gp := mp.curg
	if gp == nil || gp == mp.g0 {
		return false
	}
	// æ ‡è¯†æŠ¢å å­—æ®µ
	gp.preempt = true

	// Every call in a go routine checks for stack overflow by
	// comparing the current stack pointer to gp->stackguard0.
	// Setting gp->stackguard0 to StackPreempt folds
	// preemption into the normal stack overflow check.
	// æ›´æ–°stackguard0ï¼Œä¿è¯èƒ½æ£€æµ‹åˆ°æ ˆæº¢
	gp.stackguard0 = stackPreempt
	return true
}
```

åœ¨è¿™é‡Œï¼Œä½œè€…ä¼šæ›´æ–°   `gp.stackguard0 = stackPreempt`ï¼Œç„¶åè®©gè¯¯ä»¥ä¸ºæ ˆä¸å¤Ÿç”¨äº†ï¼Œé‚£å°±åªæœ‰ä¹–ä¹–çš„å»è¿›è¡Œæ ˆæ‰©å¼ ï¼Œç«™æ‰©å¼ çš„è¯å°±ç”¨è°ƒç”¨`newstack` åˆ†é…ä¸€ä¸ªæ–°æ ˆï¼Œç„¶åæŠŠåŸå…ˆçš„æ ˆçš„å†…å®¹æ‹·è´è¿‡å»ï¼Œè€Œåœ¨ `newstack` é‡Œé¢æœ‰ä¸€æ®µå¦‚ä¸‹

```go
if preempt {
	if thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != "" || thisg.m.p.ptr().status != _Prunning {
		// Let the goroutine keep running for now.
		// gp->preempt is set, so it will be preempted next time.
		gp.stackguard0 = gp.stack.lo + _StackGuard
		gogo(&gp.sched) // never return
	}
}
```

ç„¶åè¿™é‡Œå°±å‘ç°gè¢«æŠ¢å äº†ï¼Œé‚£ä½ æ ˆä¸å¤Ÿç”¨å°±æœ‰å¯èƒ½æ˜¯å‡çš„ï¼Œä½†æ˜¯ç®¡ä½ å‘¢ï¼Œä½ å†å»è°ƒåº¦å»å§ï¼Œä¹Ÿä¸ç»™ä½ æ‰©æ ˆäº†ï¼Œè™½ç„¶ä½œè€…å’Œé›¨ç—•å¤§ç¥éƒ½åæ§½äº†ä¸€ä¸‹è¿™ä¸ªï¼Œä½†æ˜¯è¿™ç§æŠ¢å æ–¹å¼è‡ªåŠ¨1.5ï¼ˆä¹Ÿå¯èƒ½æ›´æ—©ï¼‰å°±ä¸€ç›´å­˜åœ¨ï¼Œä¸”ç¨³å®šè¿è¡Œï¼Œå°±è¯´æ˜è¿˜æ˜¯å¾ˆç‰›é€¼çš„äº†

# æ€»ç»“

åœ¨è°ƒåº¦å™¨çš„è®¾ç½®ä¸Šï¼Œæœ€æ˜æ˜¾çš„å°±æ˜¯å¤ç”¨ï¼šg çš„freeé“¾è¡¨ï¼Œ mçš„freeåˆ—è¡¨ï¼Œ pçš„freeåˆ—è¡¨ï¼Œè¿™æ ·å°±é¿å…äº†é‡å¤åˆ›å»ºé”€æ¯é”æµªè´¹çš„èµ„æº

å…¶æ¬¡å°±æ˜¯å¤šçº§ç¼“å­˜ï¼š è¿™ä¸€å—è·Ÿå†…å­˜ä¸Šçš„è®¾è®¡æ€æƒ³ä¹Ÿæ˜¯ä¸€ç›´çš„ï¼Œpä¸€ç›´æœ‰ä¸€ä¸ª g çš„å¾…è¿è¡Œé˜Ÿåˆ—ï¼Œè‡ªå·±æ²¡æœ‰è´§è¿‡å¤šçš„æ—¶å€™ï¼Œæ‰ä¼šå¹³è¡¡åˆ°å…¨å±€é˜Ÿåˆ—ï¼Œå…¨å±€é˜Ÿåˆ—æ“ä½œéœ€è¦é”ï¼Œåˆ™æœ¬åœ°æ“ä½œåˆ™ä¸éœ€è¦ï¼Œå¤§å¤§å‡å°‘äº†é”çš„åˆ›å»ºé”€æ¯æ‰€æ¶ˆè€—çš„èµ„æº

è‡³æ­¤ï¼Œg m pçš„å…³ç³»åŠçŠ¶æ€è½¬æ¢å¤§è‡´éƒ½è®²è§£å®Œæˆäº†ï¼Œç”±äºå¯¹æ±‡ç¼–è¿™å—æ¯”è¾ƒè–„å¼±ï¼Œæ‰€ä»¥åŸºæœ¬ç•¥è¿‡äº†ï¼Œå³é¢æœ‰æœºä¼šè¿˜æ˜¯éœ€è¦å¤šäº†è§£ä¸€ç‚¹

# å‚è€ƒæ–‡æ¡£

- ã€Šgoè¯­è¨€å­¦ä¹ ç¬”è®°ã€‹

- [Golangæºç æ¢ç´¢(äºŒ) åç¨‹çš„å®ç°åŸç†](https://www.cnblogs.com/zkweb/p/7815600.html)

- [ã€Goæºç åˆ†æã€‘Go scheduler æºç åˆ†æ](https://segmentfault.com/a/1190000018777972)